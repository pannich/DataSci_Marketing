---
title: "Advertising Effects"
author: "Giovanni Compiani"
output:
  pdf_document:
    number_sections: yes
    toc: yes
header-includes: \usepackage{booktabs}
urlcolor: blue
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      fig.width = 4.5, fig.height = 3, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage

```{r}
# install.packages("RcppRoll")
library(bit64)
library(data.table)
library(RcppRoll)
library(ggplot2)
library(fixest)
library(knitr)
library(dplyr)
```

\bigskip

# Overview

In this assignment we estimate the causal short and long-run effects of advertising on demand. The assignment is closely related to the paper "TV Advertising Effectiveness and Profitability: Generalizable Results from 288 Brands" (2001, *Econometrica*) by Shapiro, Hitsch, and Tuchman.

We first combine store-level sales data from the Nielsen RMS scanner data set with DMA-level advertising exposure data from the Nielsen Ad Intel advertising data set. We then estimate ad effects based on a within-market strategy controlling for cross-sectional heterogeneity across markets.

# Data

## Brands and product modules

Data location:

```{r}
# data_folder = "C:/Users/gcompiani/Dropbox/Teaching/37105/Assignments/04 Advertising-Effects/Data"

data_folder = "Data"

brands_file = "Brands_a3.RData"
stores_file = "stores_dma.RData"
move_file  = "move_8412.RData"
adv_file = "adv_8412.RData"

```

\bigskip

The table `brands_DT` in the file `Brands_a3.RData` provides information on the available product categories (product modules) and brands, including the "focal" brands for which we may estimate advertising effects.

\medskip

```         
 product_module_code       product_module_desc   brand_code_uc        brand_descr  focal_brand
                1484   SOFT DRINKS - CARBONATED         531429        COCA-COLA R         TRUE
                8412                   ANTACIDS         621727           PRILOSEC         TRUE
                1553  SOFT DRINKS - LOW CALORIE         531433  COCA-COLA ZERO DT         TRUE
```

\bigskip

Choose Prilosec in the Antacids category for your analysis. Later, you can **optionally** repeat your analysis for the other brands.

```{r}
selected_module = 8412
selected_brand  = 621727
```

\newpage

# Data preparation

To prepare and build the data for the main analysis, load the brand and store meta-data in `Brands_a3.RData` and `stores_dma.RData`, the RMS store-level scanner (movement) data, and the Nielsen Ad Intel DMA-level TV advertising data. The scanner data and advertising data are named according to the product module, such as `move_8412.RData` and `adv_8412.RData`.

Both the RMS scanner data and the Ad Intel advertising data include information for the top four brands in the category (product module). To make our analysis computationally more manageable we will not distinguish among all individual competing brands, but instead we will aggregate all competitors into one single brand.

```{r}
load(paste0(data_folder, "/", brands_file)) 
load(paste0(data_folder, "/", stores_file))
load(paste0(data_folder, "/", move_file))
load(paste0(data_folder, "/", adv_file))
```

## RMS scanner data (`move`)

Let us start manipulating the \`move' dataset.

For consistency, rename the `units` to `quantity` and `promo_percentage` to `promotion` (use the `setnames` command). The promotion variable captures promotional activity as a continuous variable with values between 0 and 1.

```{r}
setnames(move, old = c("units", "promo_percentage"), new = c("quantity", "promotion"))
head(move)
```

Create the variable `brand_name` that we will use to distinguish between the own and aggregate competitor variables. The brand name `own` corresponds to the focal brand (Prilosec in our case), and `comp` (or any other name that you prefer) corresponds to the aggregate competitor brand.

```{r}
move[, brand_name := ifelse(brand_code_uc == selected_brand, "own", "comp")]
head(move)
```

We need to aggregate the data for each store/week observation, separately for the `own` and `comp` data. To aggregate prices and promotions we can take the simple arithmetic `mean` over all competitor brands (a weighted mean may be preferable but is not necessary in this analysis where prices and promotions largely serve as controls, not as the main marketing mix variables of interest). Aggregate quantities can be obtained as the `sum` over brand-level quantities.

```{r}
move_agg <- move[, .(quantity = sum(quantity), price = mean(price), promotion = mean(promotion)), by = .(store_code_uc, week_end, brand_name)]
head(move_agg)
```

\bigskip

Later, when we merge the RMS scanner data with the Ad Intel advertising data, we need a common key between the two data sets. This key will be provided by the DMA code and the date. Hence, we need to merge the `dma_code` found in the `stores` table with the RMS movement data.

Now merge the `dma_code` with the movement data.

```{r}
move_agg <- merge(move_agg, stores_dma[, .(store_code_uc, dma_code)], by = "store_code_uc", all.x = TRUE)
head(move_agg)
```

## Ad Intel advertising data (`adv_DT`)

The table `adv_DT` contains information on brand-level GRPs (gross rating points) for each DMA/week combination. The original data are more disaggregated, and include individual occurrences on a specific date and at a specific time and the corresponding number of impressions. `adv_DT` is based on the original data, aggregated at the DMA/week level.

Weeks are indicated by `week_end`, where the corresponding date is always a Saturday. We use Saturdays so that the `week_end` variable in the advertising data corresponds to the date convention in the RMS scanner data, where `week_end` also corresponds to a Saturday.

The data contain two variables to measure brand-level GRPs, `grp_direct` and `grp_indirect`. `grp_direct` records GRPs for which we can create a direct, unambiguous match between the brand name in the scanner data and the name of the advertised brand. Sometimes, however, it is not entirely clear if we should associate an ad in the Ad Intel data with the brand in the RMS data. For example, should we count ads for BUD LIGHT BEER LIME when measuring the GRPs that might affect sales of BUD LIGHT BEER? As such matches are somewhat debatable, we record the corresponding GRPs in the variable `grp_indirect`.

\bigskip

The data do not contain observations for all DMA/week combinations during the observation period. In particular, no DMA/week record is included if there was no corresponding advertising activity. For our purposes, however, it is important to capture that the number of GRPs was 0 for such observations. Hence, we need to "fill the gaps" in the data set.

\medskip

data.table makes it easy to achieve this goal. Let's illustrate using a simple example:

```{r}
set.seed(444)
DT = data.table(dma  = rep(LETTERS[1:2], each = 5),
                week = 1:5,
                x    = round(runif(10, min = 0, max =20)))
DT = DT[-c(2, 5, 9)]
DT
```

\medskip

In `DT`, the observations for weeks 2 and 5 in market A and week 4 in market B are missing.

To fill the holes, we need to key the data.table to specify the dimensions---here the `dma` and `week`. Then we perform a *cross join* using `CJ` (see `?CJ`). In particular, for each of the variables along which `DT` is keyed we specify the full set of values that the final data.table should contain. In this example, we want to include the markets A and B and all weeks, 1-5.

```{r}
setkey(DT, dma, week)
DT = DT[CJ(c("A", "B"), 1:5)]
DT
```

\medskip

We can replace all missing values (`NA`) with another value, say -111, like this:

```{r}
DT[is.na(DT)] = -111
DT
```

\bigskip

Use this technique to expand the advertising data in `adv_DT`, using a cross join along along all `brands`, `dma_codes`, and `weeks`:

```{r, eval = FALSE}
brands    = unique(adv_DT$brand_code_uc)
dma_codes = unique(adv_DT$dma_code)
weeks     = seq(from = min(adv_DT$week_end), to = max(adv_DT$week_end), by = "week")
```

Now perform the cross join and set missing values to 0.

```{r}
setkey(adv_DT, dma_code, week_end, brand_code_uc)
adv_DT_all <- adv_DT[CJ(dma_code = dma_codes, week_end = weeks, brand_name = brands)]

adv_DT_all[is.na(grp_direct), grp_direct := 0]
adv_DT_all[is.na(grp_indirect), grp_indirect := 0]
```

\bigskip

Create own and competitor names, and then aggregate the data at the DMA/week level, similar to what we did with the RMS scanner data. In particular, aggregate based on the sum of GRPs (separately for `grp_direct` and `grp_indirect`).

```{r}
adv_DT_all[, brand_name := ifelse(brand_code_uc == selected_brand, "own", "comp")]
adv_DT_all <- adv_DT_all[, .(grp_direct = sum(grp_direct), grp_indirect = sum(grp_indirect)), by = .(dma_code, week_end, brand_name)]
adv_DT_all[, grp := grp_direct + grp_indirect]
head(adv_DT_all)
```

\bigskip

At this stage we need to decide if we want to measure GRPs using only `grp_direct` or also including `grp_indirect`. I propose to take the broader measure, and sum the GRPs from the two variables to create a combined `grp` measure. You can later check if your results are robust if you use `grp_direct` only (this robustness analysis is optional).

*Note*: In the Antacids category, `grp_indirect` only contains the value 0 and is therefore not relevant. However, if you work with the data in the other categories, `grp_indirect` contains non-zero values.

\newpage

## Calculate adstock/goodwill

Advertising is likely to have long-run effects on demand. Hence, we will calculate adstock or goodwill variables for own and competitor advertising. We will use the following, widely-used adstock specification ($a_t$ is advertising in period $t$): $$g_{t} = \sum_{l=0}^{L}\delta^{l}\log(1+a_{t-l}) = \log(1+a_{t})+\delta \log(1+a_{t-1})+\dots+\delta^{L}\log(1+a_{t-L})$$

We add 1 to the advertising levels (GRPs) before taking the log to deal with the large number of zeros in the GRP data.

\medskip

Here is a particularly easy and fast approach to calculate adstocks. First, define the adstock parameters---the number of lags and the carry-over factor $\delta$.

```{r}
N_lags = 52
delta  = 0.7
```

Then calculate the geometric weights based on the carry-over factor.

```{r}
geom_weights = cumprod(c(1.0, rep(delta, times = N_lags)))
geom_weights = sort(geom_weights)
tail(geom_weights)
```

\medskip

Now we can calculate the adstock variable using the `roll_sum` function in the `RcppRoll` package.

```{r, eval = FALSE}
setkey(adv_DT_all, brand_name, dma_code, week_end)
adv_DT_all[, adstock := roll_sum(log(1+grp), n = N_lags+1, weights = geom_weights,
                             normalize = FALSE, align = "right",  fill = NA),
        by = .(brand_name, dma_code)]
```

\medskip

Explanations:

1.  Key the table along the cross-sectional units (brand name and DMA), then along the time variable. This step is *crucial*! If the table is not correctly sorted, the time-series order of the advertising data will be incorrect.

2.  Use the `roll_sum` function based on `log(1+grp)`. `n` indicates the total number of elements in the rolling sum, and `weights` indicates the weights for each element in the sum. `normalize = FALSE` tells the function to leave the `weights` untouched, `align = "right"` indicates to use all data above the current row in the data table to calculate the sum, and `fill = NA` indicates to fill in missing values for the first rows for which there are not enough elements to take the sum.

\medskip

Alternatively, you could code your own weighted sum function:

```{r}
weightedSum <- function(x, w) {
   T = length(x)
   L = length(w) - 1
   y = rep_len(NA, T)
   for (i in (L+1):T) y[i] = sum(x[(i-L):i]*w)
   return(y)
}
```

\bigskip

Let's compare the execution speed:

```{r, eval = FALSE}
time_a = system.time(adv_DT_all[, stock_a := weightedSum(log(1+grp), geom_weights),
                             by = .(brand_name, dma_code)])
```

```{r, eval = FALSE}
time_b = system.time(adv_DT_all[, stock_b := roll_sum(log(1+grp), n = N_lags+1,
                                                  weights = geom_weights,
                                                  normalize = FALSE,
                                                  align = "right",  fill = NA),
                             by = .(brand_name, dma_code)])
```

Even though the `weightedSum` function is fast, the speed difference with respect to the optimized code in `RcppRoll` is large.

```{r, eval = FALSE}
(time_a/time_b)[3]
```

\bigskip

Lesson: Instead of reinventing the wheel, spend a few minutes searching the Internet to see if someone has already written a package that solves your coding problems.

\newpage

## Merge scanner and advertising data

Merge (join) the advertising data with the scanner data based on brand name, DMA code, and week.

```{r}
# Check the structure of the scanner data (move_agg)
str(move_agg)

# Check the structure of the advertising data (adv_DT_all)
str(adv_DT_all)

# Ensure that the `week_end` columns in both datasets are of Date type
move_agg[, week_end := as.Date(week_end)]
adv_DT_all[, week_end := as.Date(week_end)]

```

```{r}
# Perform the merge
merged_data <- merge(adv_DT_all, move_agg, 
                     by = c("brand_name", "dma_code", "week_end"), 
                     all.x = TRUE,  
                     all.y = FALSE) 

# Check the result of the merge
head(merged_data)

```

```{r}
# No need ?? 
# Replace missing adstock values with zero
# merged_data[is.na(adstock), adstock := 0]

# Replace missing GRP values with zero as well
# merged_data[is.na(grp), grp := 0]

# Disregard the missing store_code_uc rows
merged_data <- merged_data[complete.cases(store_code_uc)]

# Find rows where the combination is duplicated
duplicates <- merged_data[duplicated(merged_data[, .(store_code_uc, week_end, brand_name)]), ]
duplicates
```

## Reshape the data

Use `dcast` to reshape the data from long to wide format. The store code and week variable are the main row identifiers. Quantity, price, promotion, and adstock are the column variables.

If you inspect the data you will see many missing `adstock` values, because the adstock variable is not defined for the first `N_lags` weeks in the data. To free memory, remove all missing values from `move` (`complete.cases`).

```{r}
dim(merged_data) # before merged

# Reshape the data from long to wide format using dcast
reshaped_data <- dcast(merged_data, store_code_uc + week_end ~ brand_name, 
                       value.var = c("quantity", "price", "promotion", "adstock", "grp"))

```

```{r}
# Remove rows with missing values using complete.cases
cleaned_data <- reshaped_data[complete.cases(reshaped_data)]

# Check the cleaned data
summary(cleaned_data)

```

```{r}
# Check the structure of the cleaned dataset
str(cleaned_data)

# View a sample of the cleaned data
head(cleaned_data)

```

## Time fixed effects

Create an index for each month/year combination in the data using the following code:

```{r}
library(lubridate)

# Ensure the 'week_end' column is in Date format
cleaned_data[, week_end := as.Date(week_end)]

# Create a month index based on the year and month of the 'week_end' column
cleaned_data[, month_index := 12 * (year(week_end) - 2011) + month(week_end)]

# View a sample of the cleaned dataset with the new month_index
head(cleaned_data)

```

```{r}
# Verify the range of month_index values
summary(cleaned_data$month_index)

# Check for a sample of the month_index column alongside the week_end column
cleaned_data[, .(week_end, month_index)][1:10]

```

\newpage

# Data inspection

## Time-series of advertising levels

We now take a look at the advertising data. First, pick a DMA. You can easily get a list of all DMA names and codes from the `stores` table. I picked `"CHICAGO IL"`, which corresponds to `dma_code` 602. Then plot the time-series of weekly GRPs for your chosen market, separately for the own and competitor brand.

Note: I suggest you create a facet plot to display the time-series of GRPs for the two brands. Use the `facet_grid` or `facet_wrap` layer as explained in the ggplot2 guide (see "More on facetting").

```{r}
# check store 602 
unique(stores_dma[, .(dma_code, dma_descr)])
```

```{r}
merged_dma_data <- merge(cleaned_data, stores_dma, by = "store_code_uc", all.x = TRUE)
# Filter the data for the selected DMA (Chicago with dma_code 602)
chicago_data <- merged_dma_data[dma_code == 602, ]
```

```{r}
# Plot time-series of weekly adstock for 'own' and 'comp'
ggplot(chicago_data, aes(x = week_end)) +
  geom_line(aes(y = grp_own, color = "Own Brand")) +
  geom_line(aes(y = grp_comp, color = "Competitor Brand")) +
  labs(
    title = "Weekly Adstock for Own and Competitor Brands",
    x = "Week End Date",
    y = "Adstock",
    color = "Brand"
  ) +
  theme_minimal()
```
```{r}
long_data <- melt(
  chicago_data, 
  id.vars = c("store_code_uc", "week_end", "dma_code", "dma_descr", "month_index"), 
  measure.vars = list(grp = c("grp_comp", "grp_own")), 
  variable.name = "brand_type", 
  value.name = "grp"
)
```

```{r}
long_data[, brand_type := ifelse(brand_type == "grp_comp", "Competitor", "Own")]

ggplot(long_data, aes(x = week_end, y = grp, color = brand_type)) +
  geom_line() +
  labs(
    title = "Weekly GRPs for Own and Competitor Brands",
    x = "Week End Date",
    y = "GRPs",
    color = "Brand Type" 
  ) +
  facet_wrap(~ brand_type) +  # Facet by brand type (Own vs Competitor)
  theme_minimal()
```

```{r}
# Calculate the DMA-level mean of grp and create normalized_grp
chicago_data <- chicago_data %>%
  group_by(dma_code) %>%
  mutate(mean_grp = mean(c(grp_own, grp_comp), na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(
    normalized_grp_own = 100 * grp_own / mean_grp,
    normalized_grp_comp = 100 * grp_comp / mean_grp
  )
```

```{r}
# Reshape to long format for plotting both variables
library(tidyverse)

long_normalized <- chicago_data %>%
  select(dma_code, normalized_grp_own, normalized_grp_comp) %>%
  pivot_longer(cols = starts_with("normalized_grp"), names_to = "brand_type", values_to = "normalized_grp")
```

```{r}
# Plot histogram of normalized_grp for both own and competitor
ggplot(long_normalized, aes(x = normalized_grp, fill = brand_type)) +
  geom_histogram(binwidth = 5, alpha = 0.7, position = "identity") +
  labs(
    title = "Histogram of Normalized GRP",
    x = "Normalized GRP (%)",
    y = "Frequency",
    fill = "Brand Type"
  ) +
  scale_x_continuous(limits = c(0, 200)) +  # Set x-axis limits to exclude extreme values
  facet_wrap(~ brand_type) +
  theme_minimal() + 
  theme(
    plot.title = element_text(size = 6),
    axis.title.x = element_text(size = 6),
    axis.title.y = element_text(size = 6),
    axis.text.x = element_text(size = 6, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 6),
    legend.title = element_text(size = 6),
    legend.text = element_text(size = 6)
  )
```
## Overall advertising variation

Create a new variable **at the DMA-level**, `normalized_grp`, defined as `100*grp/mean(grp)`. This variable captures the percentage deviation of the GRP observations relative to the DMA-level mean of advertising. Plot a histogram of `normalized_grp`.

\medskip

Note: To visualize the data you should use the `scale_x_continuous` layer to set the axis `limits`. This data set is one of many examples where some extreme outliers distort the graph.

\newpage

# Advertising effect estimation

Estimate the following specifications:

1.  Base specification that uses the log of `1+quantity` as output and the log of prices (own and competitor) and promotions as inputs. Control for store and month/year fixed effects.

2.  Add the `adstock` (own and competitor) to specification 1.

3.  Like specification 2., but not controlling for time fixed effects.

Combine the results using `etable` and comment on the results.
