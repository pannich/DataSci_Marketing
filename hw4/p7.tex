% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{fontspec}
\setmainfont{Times New Roman}
\usepackage{longtable}
\usepackage{pdflscape}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Churn Management},
  pdfauthor={Giovanni Compiani},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Churn Management}
\author{Giovanni Compiani}
\date{}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\setlength{\parskip}{6pt}
\newpage

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(bit64)}
\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(kableExtra)}
\end{Highlighting}
\end{Shaded}

\section{Overview}\label{overview}

Cell2Cell, a wireless telecommunications company (with its name altered
for confidentiality), is working to reduce customer churn. The objective
of this project is to create a model that predicts customer churn at
Cell2Cell and to leverage insights from the model to design a targeted
incentive strategy aimed at decreasing churn rates.

In the assignment, you will address these key issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Is customer churn at Cell2Cell predictable from the customer
  information that Cell2Cell maintains?
\item
  What factors drive customer churn? Which factors are particularly
  important?
\item
  What incentives should Cell2Cell offer to prevent customer churn?
\item
  What is the economic value of a proposed targeted plan to prevent
  churn, and how does this value differ across customer segments?
  Compare the economic value to an incentive with a cost of \$100 and
  another incentive with a cost of \$175. Which customers segments
  should receive the incentive? Does the answer depend on the success
  probability?
\end{enumerate}

Note that, in what follows, the key steps you need to take are
highlighted in \emph{italic}.

\newpage

\section{Data}\label{data}

All data are contained in the file \texttt{Cell2Cell.RData}, which is
posted on Canvas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_folder    }\OtherTok{=} \StringTok{"./Data"}
\NormalTok{Cell2Cell }\OtherTok{=} \StringTok{"Cell2Cell.RData"}
\FunctionTok{load}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(data\_folder, }\StringTok{"/"}\NormalTok{, Cell2Cell))}
\end{Highlighting}
\end{Shaded}

\medskip

Please consult the file \texttt{Cell2Cell-Database-Documentation.xlsx}
for a description of the data and some summary statistics. Note that
\emph{calibration sample} is an alternative term for \emph{training} or
\emph{estimation} sample.

\emph{Report the churn rate in the calibration sample and in the
validation sample and compare the two.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# str(cell2cell\_DT)}

\NormalTok{calibration\_sample }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(cell2cell\_DT, calibrat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{validation\_sample }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(cell2cell\_DT, calibrat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\CommentTok{\# Calculate churn rate in the calibration sample}
\NormalTok{calibration\_churn\_rate }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(calibration\_sample}\SpecialCharTok{$}\NormalTok{churn)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Churn Rate in Calibration Sample:"}\NormalTok{, calibration\_churn\_rate, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Churn Rate in Calibration Sample: 0.5 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate churn rate in the validation sample}
\NormalTok{validation\_churn\_rate }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(validation\_sample}\SpecialCharTok{$}\NormalTok{churn)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Churn Rate in Validation Sample:"}\NormalTok{, validation\_churn\_rate, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Churn Rate in Validation Sample: 0.01961542 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compare churn rates}
\ControlFlowTok{if}\NormalTok{ (calibration\_churn\_rate }\SpecialCharTok{\textgreater{}}\NormalTok{ validation\_churn\_rate) \{}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"The churn rate is higher in the calibration sample.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (calibration\_churn\_rate }\SpecialCharTok{\textless{}}\NormalTok{ validation\_churn\_rate) \{}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"The churn rate is higher in the validation sample.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"The churn rates are the same in both samples.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The churn rate is higher in the calibration sample.
\end{verbatim}

You can see that the calibration sample was selected using
\emph{oversampling}. The purpose of oversampling was to obtain more
precise estimates (lower standard errors) when estimating a logistic
regression model. The validation sample, on the other hand, was not
created using oversampling and represents the \emph{true churn rate} in
the data.

As you can see, some variables have missing values, which---as you know
by now---is common and of no concern (unless the missing values indicate
some \emph{systematic} flaws or bias in how the data were constructed).
Most estimation methods in R will automatically delete rows with missing
values before estimating the model. However, the \texttt{predict}
methods will yield \texttt{NA} values if a row in the data used for
prediction contains missing values. Hence, in a situation where you
don't need to keep the full data I recommend to remove any observations
with missing values before you conduct the main analysis.

\emph{Perform this data-cleaning step.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove rows with missing values from the data}
\NormalTok{cell2cell\_clean }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(cell2cell\_DT)}

\CommentTok{\# Verify that missing values have been removed}
\FunctionTok{print}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Original dataset had \%d rows"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(cell2cell\_DT)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Original dataset had 71047 rows"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Cleaned dataset has \%d rows"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(cell2cell\_clean)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Cleaned dataset has 69309 rows"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Removed \%d rows with missing values"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(cell2cell\_DT) }\SpecialCharTok{{-}} \FunctionTok{nrow}\NormalTok{(cell2cell\_clean)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Removed 1738 rows with missing values"
\end{verbatim}

\newpage

\section{Model estimation}\label{model-estimation}

\emph{Estimate a logit model to predict the conditional churn
probability.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the logistic regression model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
             \AttributeTok{data =}\NormalTok{ cell2cell\_clean[, .SD, }\AttributeTok{.SDcols =} \SpecialCharTok{!}\FunctionTok{c}\NormalTok{(}\StringTok{"customer"}\NormalTok{, }\StringTok{"calibrat"}\NormalTok{)], }
             \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

You can inspect the regression output using methods you already used,
such as \texttt{summary}. Having said this, especially when you have a
large number of inputs, it can be convenient to store the regression
estimates in a table. A simple way to do this is to install the
\href{https://cran.r-project.org/web/packages/broom/vignettes/broom.html}{broom
package} that has the purpose of cleaning up messy R output.

Using the \texttt{tidy} function in the \texttt{broom} package it is
trivial to capture the regression output in the form of a data.table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tidy the model output and convert to data.table}
\NormalTok{results\_DT }\OtherTok{\textless{}{-}} \FunctionTok{as.data.table}\NormalTok{(}\FunctionTok{tidy}\NormalTok{(fit))}

\CommentTok{\# Display the regression results with 5 decimal places}
\FunctionTok{kable}\NormalTok{(results\_DT, }\AttributeTok{digits =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
term & estimate & std.error & statistic & p.value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(Intercept) & -0.82035 & 0.07826 & -10.48202 & 0.00000 \\
revenue & 0.00160 & 0.00066 & 2.42628 & 0.01525 \\
mou & -0.00028 & 0.00004 & -6.57237 & 0.00000 \\
recchrge & -0.00263 & 0.00074 & -3.55962 & 0.00037 \\
directas & 0.00023 & 0.00477 & 0.04781 & 0.96187 \\
overage & 0.00087 & 0.00023 & 3.73561 & 0.00019 \\
roam & 0.00281 & 0.00120 & 2.34317 & 0.01912 \\
changem & -0.00057 & 0.00005 & -12.57609 & 0.00000 \\
changer & 0.00290 & 0.00030 & 9.62096 & 0.00000 \\
dropvce & 0.00801 & 0.00698 & 1.14770 & 0.25109 \\
blckvce & 0.00345 & 0.00692 & 0.49930 & 0.61757 \\
unansvce & 0.00082 & 0.00037 & 2.23354 & 0.02551 \\
custcare & -0.00650 & 0.00230 & -2.82186 & 0.00477 \\
threeway & -0.03574 & 0.00969 & -3.68748 & 0.00023 \\
mourec & 0.00016 & 0.00011 & 1.39133 & 0.16413 \\
outcalls & 0.00056 & 0.00049 & 1.14705 & 0.25136 \\
incalls & -0.00178 & 0.00090 & -1.97778 & 0.04795 \\
peakvce & -0.00063 & 0.00018 & -3.41461 & 0.00064 \\
opeakvce & -0.00018 & 0.00022 & -0.82605 & 0.40878 \\
dropblk & -0.00015 & 0.00683 & -0.02230 & 0.98221 \\
callfwdv & -0.01786 & 0.02199 & -0.81229 & 0.41663 \\
callwait & -0.00008 & 0.00258 & -0.03224 & 0.97428 \\
months & -0.02227 & 0.00168 & -13.24110 & 0.00000 \\
uniqsubs & 0.17910 & 0.01576 & 11.36467 & 0.00000 \\
actvsubs & -0.18508 & 0.02251 & -8.22308 & 0.00000 \\
phones & 0.04958 & 0.01494 & 3.31785 & 0.00091 \\
models & 0.01377 & 0.02321 & 0.59341 & 0.55290 \\
eqpdays & 0.00142 & 0.00006 & 23.00599 & 0.00000 \\
age1 & -0.00409 & 0.00072 & -5.69500 & 0.00000 \\
age2 & -0.00093 & 0.00056 & -1.65423 & 0.09808 \\
children & 0.10973 & 0.02330 & 4.71042 & 0.00000 \\
credita & -0.17827 & 0.02964 & -6.01401 & 0.00000 \\
creditaa & -0.39057 & 0.02924 & -13.35695 & 0.00000 \\
prizmrur & 0.12196 & 0.04067 & 2.99893 & 0.00271 \\
prizmub & -0.03453 & 0.02019 & -1.71005 & 0.08726 \\
prizmtwn & 0.04801 & 0.02580 & 1.86104 & 0.06274 \\
refurb & 0.25268 & 0.02617 & 9.65444 & 0.00000 \\
webcap & -0.13360 & 0.03013 & -4.43361 & 0.00001 \\
truck & 0.03195 & 0.02968 & 1.07643 & 0.28173 \\
rv & 0.01306 & 0.03957 & 0.33005 & 0.74136 \\
occprof & -0.01636 & 0.02689 & -0.60840 & 0.54292 \\
occcler & 0.08278 & 0.06123 & 1.35196 & 0.17639 \\
occcrft & -0.02871 & 0.05222 & -0.54977 & 0.58248 \\
occstud & 0.07535 & 0.09816 & 0.76766 & 0.44269 \\
occhmkr & 0.11401 & 0.14800 & 0.77031 & 0.44112 \\
occret & -0.05320 & 0.07590 & -0.70091 & 0.48336 \\
occself & -0.01679 & 0.06743 & -0.24896 & 0.80339 \\
ownrent & 0.04038 & 0.03541 & 1.14025 & 0.25418 \\
marryun & 0.09073 & 0.02811 & 3.22759 & 0.00125 \\
marryyes & 0.05576 & 0.02686 & 2.07578 & 0.03791 \\
mailord & -0.04646 & 0.07135 & -0.65115 & 0.51495 \\
mailres & -0.09220 & 0.07162 & -1.28725 & 0.19801 \\
mailflag & 0.01346 & 0.07106 & 0.18935 & 0.84982 \\
travel & 0.00532 & 0.03914 & 0.13598 & 0.89184 \\
pcown & 0.02450 & 0.02556 & 0.95851 & 0.33781 \\
creditcd & 0.08633 & 0.03585 & 2.40792 & 0.01604 \\
retcalls & 0.18640 & 0.14799 & 1.25950 & 0.20785 \\
retaccpt & -0.17696 & 0.08357 & -2.11759 & 0.03421 \\
newcelly & -0.03518 & 0.02261 & -1.55586 & 0.11974 \\
newcelln & 0.02663 & 0.02596 & 1.02601 & 0.30489 \\
refer & -0.08079 & 0.03543 & -2.28011 & 0.02260 \\
incmiss & -0.07086 & 0.04938 & -1.43484 & 0.15133 \\
income & -0.00712 & 0.00499 & -1.42673 & 0.15366 \\
mcycle & 0.09062 & 0.07317 & 1.23859 & 0.21550 \\
setprcm & -0.09003 & 0.03317 & -2.71375 & 0.00665 \\
setprc & 0.00047 & 0.00023 & 1.99417 & 0.04613 \\
retcall & 0.64385 & 0.15547 & 4.14117 & 0.00003 \\
\end{longtable}

For \texttt{kable} to work, you need to load the \texttt{knitr} library.

\newpage

\section{Prediction: Accounting for
oversampling}\label{prediction-accounting-for-oversampling}

The idea of oversampling is as follows. If the response rate in the data
is small, there is a strong imbalance between observations with a
response of \(Y=1\) and a response of \(Y=0\). As a consequence,
estimating the model is difficult and the estimates will be imprecise,
i.e.~they will have large standard errors.

The solution: Create a training sample with one half of observations
randomly chosen from the original data with response \(Y=1\), and the
other half randomly chosen from the original data with response \(Y=0\).
Now estimation is easier and the standard errors will be smaller.

However, when applied to logistic regression, oversampling will result
in an inconsistent estimate of the intercept (constant) term, although
all other estimates will be consistent. Hence, if we do not de-bias
(adjust) the intercept, the predicted probabilities will be too large,
reflecting the artificial response rate of \(\frac{1}{2}\) in the
over-sampled training data.

In order to de-bias the scale of the predicted response (in this
example: churn) in the validation sample we need to supply an
\emph{offset variable} to the logistic regression model. An offset is a
known number that is added to the right-hand side of the regression when
estimating the model, and adding the offset will correspondingly change
the estimate of the intercept. The offset takes the form:
\[\text{offset}=\left[\log(\bar{p}_{t})-\log(1-\bar{p}_{t})\right]-\left[\log(\bar{p}_{v})-\log(1-\bar{p}_{v})\right]\]

Here, \(\bar{p}_{t}\) is the average response rate in the training
sample and \(\bar{p}_{v}\) is the average response rate in the
validation sample. Note that the offset is positive (given that
\(\bar{p}_t > \bar{p}_v\)), so that including the offset term when
estimating the model accounts for the fact that the training sample has
a higher share of \(Y=1\) relative to the validation sample.

Conversely, when we predict the response rate in the validation sample,
we set the offset variable to 0.

Why does this work? --- Conceptually, logistic regression is a
regression model for the log-odds of the response (outcome) probability,
\[\log \left(\frac{p}{1-p}\right) = \log(p) - \log(1-p) = \beta_0 + \beta_1 X_1 + \beta_2 X_1 + \dots \]

When we add the offset variable to the right hand side of the regression
model the estimation algorithm will ``incorporate'' the offset in the
intercept, \(\beta_0\). The effect of setting the offset to 0 (when
applying the model to the validation sample) is equivalent to
subtracting the offset from the intercept. Subtracting the offset
amounts to:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Subtracting \(\log(\bar{p}_{t})-\log(1-\bar{p}_{t})\), the log-odds of
  the artificial response rate in the training sample, and
\item
  Adding \(\log(\bar{p}_{v})-\log(1-\bar{p}_{v})\), the log-odds in the
  validation sample that reflects the true log-odds in the data.
\end{enumerate}

This process de-biases the predicted response, i.e.~restores the correct
response level in the validation sample.

Note: Never use over-sampling to create the validation sample, otherwise
the offset variable approach will not work.

\bigskip

\emph{Create an \texttt{offset\_var} variable and add it to the data
set. Then re-estimate the logistic regression. To tell \texttt{glm} that
you want to use \texttt{offset\_var}, you need to use a formula of the
form:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{offset}\NormalTok{(offset\_var) }\SpecialCharTok{+} \ErrorTok{\textless{}}\NormalTok{all other variables}\SpecialCharTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Oversampling Bias: Helps in training but inflates the response rate, requiring adjustment.}
\CommentTok{\# Offset Variable: Corrects the intercept to align predictions with the true response rate.}
\CommentTok{\# Validation Data: Always use non{-}oversampled data for evaluation and prediction.}

\CommentTok{\# Separate calibration and validation samples}
\NormalTok{calibration\_sample }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(cell2cell\_clean, calibrat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{validation\_sample }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(cell2cell\_clean, calibrat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\CommentTok{\# Calculate the average churn rates in each sample}
\NormalTok{pt }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(calibration\_sample}\SpecialCharTok{$}\NormalTok{churn)}
\NormalTok{pv }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(validation\_sample}\SpecialCharTok{$}\NormalTok{churn)}

\CommentTok{\# Calculate the offset variable}
\NormalTok{offset\_var }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{log}\NormalTok{(pt) }\SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pt)) }\SpecialCharTok{{-}}\NormalTok{ (}\FunctionTok{log}\NormalTok{(pv) }\SpecialCharTok{{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pv))}

\CommentTok{\# Add the offset variable to the calibration sample}
\NormalTok{calibration\_sample}\SpecialCharTok{$}\NormalTok{offset\_var }\OtherTok{\textless{}{-}}\NormalTok{ offset\_var}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the logistic regression model with the offset}
\NormalTok{fit\_with\_offset }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{+} \FunctionTok{offset}\NormalTok{(offset\_var), }
                       \AttributeTok{data =}\NormalTok{ calibration\_sample[, .SD, }\AttributeTok{.SDcols =} \SpecialCharTok{!}\FunctionTok{c}\NormalTok{(}\StringTok{"customer"}\NormalTok{, }\StringTok{"calibrat"}\NormalTok{)],}
                       \AttributeTok{family =}\NormalTok{ binomial)}

\CommentTok{\# View the summary of the model}
\FunctionTok{summary}\NormalTok{(fit\_with\_offset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = churn ~ . + offset(offset_var), family = binomial, 
    data = calibration_sample[, .SD, .SDcols = !c("customer", 
        "calibrat")])

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.771e+00  9.527e-02 -39.583  < 2e-16 ***
revenue      1.963e-03  7.981e-04   2.460 0.013888 *  
mou         -2.809e-04  4.965e-05  -5.657 1.54e-08 ***
recchrge    -3.123e-03  8.888e-04  -3.513 0.000443 ***
directas    -1.196e-03  5.939e-03  -0.201 0.840425    
overage      7.602e-04  2.804e-04   2.711 0.006704 ** 
roam         7.091e-03  2.064e-03   3.436 0.000589 ***
changem     -4.919e-04  5.351e-05  -9.194  < 2e-16 ***
changer      2.303e-03  3.687e-04   6.247 4.20e-10 ***
dropvce      1.134e-02  7.254e-03   1.563 0.118044    
blckvce      6.403e-03  7.157e-03   0.895 0.371044    
unansvce     9.215e-04  4.478e-04   2.058 0.039609 *  
custcare    -5.951e-03  2.553e-03  -2.331 0.019738 *  
threeway    -3.029e-02  1.125e-02  -2.691 0.007122 ** 
mourec       1.339e-04  1.316e-04   1.018 0.308816    
outcalls     1.119e-03  5.906e-04   1.894 0.058164 .  
incalls     -3.107e-03  1.058e-03  -2.937 0.003314 ** 
peakvce     -6.696e-04  2.190e-04  -3.058 0.002229 ** 
opeakvce    -2.080e-04  2.657e-04  -0.783 0.433722    
dropblk     -3.115e-03  7.039e-03  -0.442 0.658135    
callfwdv    -2.643e-03  2.315e-02  -0.114 0.909128    
callwait     2.085e-03  3.141e-03   0.664 0.506825    
months      -2.128e-02  1.998e-03 -10.652  < 2e-16 ***
uniqsubs     1.844e-01  1.999e-02   9.225  < 2e-16 ***
actvsubs    -2.057e-01  2.791e-02  -7.372 1.68e-13 ***
phones       4.866e-02  1.817e-02   2.678 0.007398 ** 
models       1.380e-02  2.787e-02   0.495 0.620596    
eqpdays      1.442e-03  7.466e-05  19.309  < 2e-16 ***
age1        -3.303e-03  8.723e-04  -3.787 0.000152 ***
age2        -1.168e-03  6.800e-04  -1.718 0.085778 .  
children     9.455e-02  2.815e-02   3.359 0.000782 ***
credita     -1.781e-01  3.550e-02  -5.016 5.28e-07 ***
creditaa    -3.626e-01  3.458e-02 -10.488  < 2e-16 ***
prizmrur     6.649e-02  4.956e-02   1.342 0.179746    
prizmub     -3.963e-02  2.441e-02  -1.624 0.104400    
prizmtwn     4.622e-02  3.145e-02   1.470 0.141602    
refurb       2.340e-01  3.196e-02   7.323 2.42e-13 ***
webcap      -1.561e-01  3.756e-02  -4.157 3.23e-05 ***
truck        2.689e-02  3.600e-02   0.747 0.455077    
rv           1.186e-02  4.801e-02   0.247 0.804909    
occprof     -1.987e-02  3.250e-02  -0.611 0.540996    
occcler      3.949e-02  7.491e-02   0.527 0.598053    
occcrft     -2.013e-02  6.290e-02  -0.320 0.748897    
occstud      1.200e-01  1.219e-01   0.984 0.324916    
occhmkr      2.559e-01  1.901e-01   1.346 0.178266    
occret      -3.993e-02  9.055e-02  -0.441 0.659244    
occself     -7.057e-02  8.059e-02  -0.876 0.381215    
ownrent      2.554e-03  4.272e-02   0.060 0.952328    
marryun      1.088e-01  3.403e-02   3.198 0.001385 ** 
marryyes     5.570e-02  3.249e-02   1.714 0.086444 .  
mailord      7.687e-04  8.565e-02   0.009 0.992840    
mailres     -1.297e-01  8.604e-02  -1.508 0.131672    
mailflag    -4.818e-02  8.445e-02  -0.571 0.568303    
travel      -5.320e-04  4.732e-02  -0.011 0.991030    
pcown        3.418e-02  3.096e-02   1.104 0.269633    
creditcd     4.202e-02  4.371e-02   0.961 0.336435    
retcalls     1.203e-02  1.837e-01   0.066 0.947760    
retaccpt    -1.279e-01  1.076e-01  -1.188 0.234909    
newcelly    -7.053e-02  2.727e-02  -2.586 0.009708 ** 
newcelln    -5.084e-03  3.153e-02  -0.161 0.871883    
refer       -5.003e-02  4.214e-02  -1.187 0.235215    
incmiss     -9.151e-02  6.006e-02  -1.524 0.127615    
income      -1.324e-02  6.035e-03  -2.195 0.028177 *  
mcycle       1.223e-01  8.898e-02   1.374 0.169302    
setprcm     -9.632e-02  4.051e-02  -2.377 0.017431 *  
setprc       6.203e-04  2.827e-04   2.194 0.028222 *  
retcall      7.937e-01  1.946e-01   4.079 4.52e-05 ***
offset_var          NA         NA      NA       NA    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 53983  on 38940  degrees of freedom
Residual deviance: 52300  on 38874  degrees of freedom
AIC: 52434

Number of Fisher Scoring iterations: 4
\end{verbatim}

\emph{Where} you place \texttt{offset()} on the right-hand side of the
formula is irrelevant.

\medskip

\emph{Before predicting the response rate in the validation sample set
the offset to 0. Then, when you invoke the \texttt{predict} function,
supply the data with the offset set to 0 using the \texttt{newdata}
option.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add offset\_var as 0 in the validation sample}
\NormalTok{validation\_sample}\SpecialCharTok{$}\NormalTok{offset\_var }\OtherTok{\textless{}{-}} \DecValTok{0}

\CommentTok{\# Predict churn probabilities in the validation sample with offset set to 0}
\NormalTok{validation\_sample}\SpecialCharTok{$}\NormalTok{predicted\_churn }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
\NormalTok{  fit\_with\_offset,}
  \AttributeTok{newdata =}\NormalTok{ validation\_sample[, .SD, }\AttributeTok{.SDcols =} \SpecialCharTok{!}\FunctionTok{c}\NormalTok{(}\StringTok{"customer"}\NormalTok{,}\StringTok{"calibrat"}\NormalTok{)],}
  \AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
\end{verbatim}

\emph{Compare the average predicted response with the average observed
response rate in the validation sample.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the average predicted churn rate}
\NormalTok{average\_predicted\_churn }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(validation\_sample}\SpecialCharTok{$}\NormalTok{predicted\_churn)}

\CommentTok{\# Calculate the observed churn rate in the validation sample}
\NormalTok{average\_observed\_churn }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(validation\_sample}\SpecialCharTok{$}\NormalTok{churn)}

\CommentTok{\# Display the results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Average Predicted Churn Rate in Validation Sample:"}\NormalTok{, average\_predicted\_churn, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Average Predicted Churn Rate in Validation Sample: 0.01943244 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Average Observed Churn Rate in Validation Sample:"}\NormalTok{, average\_observed\_churn, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Average Observed Churn Rate in Validation Sample: 0.01929663 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"Average of predicted churn rate and the actual churn rate are close after \textbackslash{}}
\StringTok{    de{-}biasing with the offset varaible. This indicates that the model\textquotesingle{}s \textbackslash{}}
\StringTok{    predictions are correctly calibrated to reflect the real{-}world churn rate. }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Average of predicted churn rate and the actual churn rate are close after 
    de-biasing with the offset varaible. This indicates that the model's 
    predictions are correctly calibrated to reflect the real-world churn rate. 
\end{verbatim}

\newpage

\section{Predictive power: Lift}\label{predictive-power-lift}

We evaluate the predictive fit of the logistic regression model using a
lift table and lift chart. To develop reusable code, we develop a
function that returns a lift table. The function (call it
\texttt{liftTable}) will need to take the following inputs:

\begin{itemize}
\tightlist
\item
  Predicted outcome or score
\item
  Observed outcome
\item
  Number of segments to be created based on the score
\end{itemize}

\texttt{liftTable} will return a data.table that contains:

\begin{itemize}
\tightlist
\item
  An index (\texttt{score\_group}) for each segment that was created
  based on the score
\item
  The average score value (predicted outcome) in the
  \texttt{score\_group}
\item
  The average observed outcome in the \texttt{score\_group}
\item
  The lift factor
\end{itemize}

\bigskip

To code the \texttt{liftTable} command, I recommend to use the
\texttt{cut\_number} function in the ggplot2 package.
\texttt{cut\_number} takes a variable \texttt{x} and creates \texttt{n}
groups with an approximately equal number of observations in each group.
Observations are assigned to the groups based on their ranking along the
variable \texttt{x}. The format is:

\begin{verbatim}
cut_number(x, n = <no. of groups>)
\end{verbatim}

\medskip

To illustrate, we draw 10,000 random numbers from a uniform distribution
on \([0,5]\). \texttt{cut\_number} assigns each number to one of five
(because we set \texttt{n\ =\ 5}) groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{DT }\OtherTok{=} \FunctionTok{data.table}\NormalTok{(}\AttributeTok{x =} \FunctionTok{runif}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{5}\NormalTok{))}
\NormalTok{DT[, group    }\SpecialCharTok{:=} \FunctionTok{cut\_number}\NormalTok{(x, }\AttributeTok{n =} \DecValTok{5}\NormalTok{)]}
\NormalTok{DT[, group\_no }\SpecialCharTok{:=} \FunctionTok{as.integer}\NormalTok{(group)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(DT)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           x        group group_no
       <num>       <fctr>    <int>
1: 1.4378876     (1,2.01]        2
2: 3.9415257  (2.98,3.98]        4
3: 2.0448846  (2.01,2.98]        3
4: 4.4150870     (3.98,5]        5
5: 4.7023364     (3.98,5]        5
6: 0.2277825 [0.000327,1]        1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(DT}\SpecialCharTok{$}\NormalTok{group)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

[0.000327,1]     (1,2.01]  (2.01,2.98]  (2.98,3.98]     (3.98,5] 
        2000         2000         2000         2000         2000 
\end{verbatim}

\medskip

As expected, because \texttt{x} is uniformly distributed on \([0,5]\),
the five groups created by \texttt{cut\_number} correspond almost
exactly to a \([k,k+1]\) interval (\(k=0,1,\dots,4\)), and each of these
intervals contains exactly 20 percent of all observations based on the
rank of the \texttt{x} values. The \texttt{group} variable that we
created is a factor that we converted to an integer score.

\bigskip

\emph{Calculate a lift table for 20 segments. Inspect the lift table.
Then provide two charts. First, plot the \texttt{score\_group} segments
on the x-axis versus the observed churn rate on the y-axis. Second, plot
the segments versus the lift factor, and add a horizontal line at}
\(y=100\). How to do this in ggplot2 is explained in the ggplot2 guide
(look for the \texttt{yintercept} option).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{liftTable }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(predicted, observed, }\AttributeTok{segments =} \DecValTok{20}\NormalTok{) \{}
  \CommentTok{\# Combine inputs into a data.table}
\NormalTok{  DT }\OtherTok{\textless{}{-}} \FunctionTok{data.table}\NormalTok{(}\AttributeTok{predicted =}\NormalTok{ predicted, }\AttributeTok{observed =}\NormalTok{ observed)}
  
  \CommentTok{\# Create score groups using cut\_number}
\NormalTok{  DT[, score\_group }\SpecialCharTok{:=} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{cut\_number}\NormalTok{(predicted, }\AttributeTok{n =}\NormalTok{ segments))]}
  
  \CommentTok{\# Calculate lift table metrics}
\NormalTok{  lift\_table }\OtherTok{\textless{}{-}}\NormalTok{ DT[, .(}
    \AttributeTok{avg\_score =} \FunctionTok{mean}\NormalTok{(predicted),}
    \AttributeTok{avg\_observed =} \FunctionTok{mean}\NormalTok{(observed),}
    \AttributeTok{lift\_factor =} \FunctionTok{mean}\NormalTok{(observed) }\SpecialCharTok{/} \FunctionTok{mean}\NormalTok{(DT}\SpecialCharTok{$}\NormalTok{observed) }\SpecialCharTok{*} \DecValTok{100}
\NormalTok{  ), by }\OtherTok{=}\NormalTok{ score\_group]}
  
  \FunctionTok{return}\NormalTok{(lift\_table)}
\NormalTok{\}}

\CommentTok{\# Apply the liftTable function}
\NormalTok{lift\_table }\OtherTok{\textless{}{-}} \FunctionTok{liftTable}\NormalTok{(}
  \AttributeTok{predicted =}\NormalTok{ validation\_sample}\SpecialCharTok{$}\NormalTok{predicted\_churn,}
  \AttributeTok{observed =}\NormalTok{ validation\_sample}\SpecialCharTok{$}\NormalTok{churn,}
  \AttributeTok{segments =} \DecValTok{20}
\NormalTok{)}

\CommentTok{\# Inspect the lift table}
\FunctionTok{print}\NormalTok{(lift\_table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    score_group   avg_score avg_observed lift_factor
          <int>       <num>        <num>       <num>
 1:           1 0.007345874  0.003291639    17.05811
 2:           2 0.009751282  0.011198946    58.03577
 3:           3 0.011117084  0.011849901    61.40918
 4:           9 0.016544030  0.017774852    92.11377
 5:          10 0.017328919  0.017127800    88.76058
 6:           5 0.013289831  0.012516469    64.86350
 7:          20 0.047617454  0.037524687   194.46241
 8:           7 0.014985665  0.016469038    85.34672
 9:          17 0.025339449  0.028326746   146.79635
10:          16 0.023520373  0.026350461   136.55474
11:          19 0.031557441  0.029644269   153.62409
12:           4 0.012293521  0.011198946    58.03577
13:          14 0.020985705  0.021739130   112.65766
14:           6 0.014176228  0.013166557    68.23242
15:          12 0.019027414  0.019749835   102.34864
16:           8 0.015772776  0.012516469    64.86350
17:          13 0.019964653  0.023715415   122.89927
18:          11 0.018149515  0.017786561    92.17445
19:          15 0.022128569  0.020408163   105.76026
20:          18 0.027746205  0.033574720   173.99268
    score_group   avg_score avg_observed lift_factor
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  Churn Rate vs. Score Groups}
\CommentTok{\# Higher score groups (with higher predicted churn probabilities) should have higher}
\CommentTok{\# observed churn rates.}
\FunctionTok{ggplot}\NormalTok{(lift\_table, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ score\_group, }\AttributeTok{y =}\NormalTok{ avg\_observed)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"steelblue"}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# Use \textasciigrave{}linewidth\textasciigrave{} for line width}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"steelblue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# Points on the line}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Observed Churn Rate by Score Group"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Score Group"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Observed Churn Rate"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{flushright}\includegraphics{p7_files/figure-latex/unnamed-chunk-14-1} \end{flushright}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lift Factor vs. Score Groups}
\CommentTok{\# The lift factor should be highest for the top score groups (high{-}risk churners).}
\CommentTok{\# The horizontal line at y = 100 represents the baseline (random guessing or avg churn rate). }
\CommentTok{\# It hits close to the mid segment group reflects that the churn predictions align }
\CommentTok{\# with the reality (observation) of churn that is expected per each segment.}
\FunctionTok{ggplot}\NormalTok{(lift\_table, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ score\_group, }\AttributeTok{y =}\NormalTok{ lift\_factor)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# Use \textasciigrave{}linewidth\textasciigrave{} for line width}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"darkorange"}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# Points on the line}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{100}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}  \CommentTok{\# Horizontal reference line}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Lift Factor by Score Group"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Score Group"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Lift Factor"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{flushright}\includegraphics{p7_files/figure-latex/unnamed-chunk-14-2} \end{flushright}

\newpage

\section{Why do customers churn? --- Effect
sizes}\label{why-do-customers-churn-effect-sizes}

We would like to understand \emph{why} customers churn, which can help
us to propose incentives to prevent customer churn

\emph{To this end, construct a table that contains comparable effect
sizes (changes in the churn probability) for all independent variables,
as we discussed in class.}

Here are a few more details on the steps needed to create this table:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Because logistic regression coefficients are not directly
  interpretable, we estimate a linear probability model of customer
  churn. In a linear probability model we regress the \(Y=0,1\) output
  on all the customer features. The estimated coefficients can be
  interpreted as differences in \(\Pr\{Y=1 | X_1, X_2, \dots\}\) for a
  one-unit difference in one of the features, \(X_k\). Note: \textbf{The
  \emph{offset variable} should not be included in the linear
  probability model as it is specific to logistic regression.}
\item
  Note that our analysis is based on a \emph{comparison} of the effect
  sizes of the different variables. However, because the variables have
  different scales, the effect sizes are not directly comparable. For
  example, \texttt{revenue} (mean monthly revenue) and \texttt{mou}
  (mean monthly minutes use) have different means and standard
  deviations, and hence the effects of increasing \texttt{revenue} and
  \texttt{mou} by one unit on the churn probabilities are not comparable
  without taking the scale differences into account.
\item
  To solve this problem we \textbf{standardize} the independent
  variables in the data. To standardize, we divide the values of each
  independent variable by its standard deviation, except if the variable
  is a 0/1 dummy. Once standardized, all variables except the dummies
  will have a standard deviation of 1, and a one unit difference
  corresponds to a one standard deviation difference in the original,
  non-standardized variable. Here's a function,
  \texttt{standardize\_columns}, that takes a column \texttt{x} as input
  and returns the standardized values of the column:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{standardize\_columns }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
   
   \CommentTok{\# Check if the column is a dummy variable}
\NormalTok{   elements }\OtherTok{=} \FunctionTok{unique}\NormalTok{(x)}
   \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(elements) }\SpecialCharTok{==} \DecValTok{2}\DataTypeTok{L} \SpecialCharTok{\&} \FunctionTok{all}\NormalTok{(elements }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\DataTypeTok{L}\NormalTok{,}\DecValTok{1}\DataTypeTok{L}\NormalTok{))) \{}
\NormalTok{      is\_dummy }\OtherTok{=} \ConstantTok{TRUE}
\NormalTok{   \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      is\_dummy }\OtherTok{=} \ConstantTok{FALSE}
\NormalTok{   \}}
   
   \CommentTok{\# If not a dummy, divide values in x by its standard deviation}
   \ControlFlowTok{if}\NormalTok{ (is\_dummy }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{) x }\OtherTok{=}\NormalTok{ x}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(x, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
   
   \FunctionTok{return}\NormalTok{(x)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The first part of the function checks that the input \texttt{x} has
exactly two elements and that these elements are the integers 0 and 1.
Note that in R, numbers are represented as floating point numbers by
default. However, adding \texttt{L} after the numbers tells R to
represent the number as an integer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "numeric"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(}\DecValTok{1}\DataTypeTok{L}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "integer"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DT\_lin\_prob }\OtherTok{=}\NormalTok{ cell2cell\_clean[calibrat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{]}

\CommentTok{\# Create a vector that contains the names of all inputs (covariates)}
\CommentTok{\# remove customer, calibrat, churn columns, retcall}
\NormalTok{all\_columns   }\OtherTok{=} \FunctionTok{names}\NormalTok{(DT\_lin\_prob)}
\NormalTok{input\_columns }\OtherTok{=}\NormalTok{ all\_columns[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\FunctionTok{length}\NormalTok{(all\_columns))] }

\CommentTok{\# Standardize all input columns}
\NormalTok{DT\_lin\_prob[, (input\_columns) }\SpecialCharTok{:=} \FunctionTok{lapply}\NormalTok{(.SD, standardize\_columns), .SDcols }\OtherTok{=}\NormalTok{ input\_columns]}

\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{Dv\_lin\_prob }\OtherTok{=}\NormalTok{ cell2cell\_clean[calibrat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{]}

\CommentTok{\# Create a vector that contains the names of all inputs (covariates)}
\NormalTok{all\_columns   }\OtherTok{=} \FunctionTok{names}\NormalTok{(Dv\_lin\_prob)}
\NormalTok{input\_columns }\OtherTok{=}\NormalTok{ all\_columns[}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\FunctionTok{length}\NormalTok{(all\_columns))]}

\CommentTok{\# Standardize all input columns}
\NormalTok{Dv\_lin\_prob[, (input\_columns) }\SpecialCharTok{:=} \FunctionTok{lapply}\NormalTok{(.SD, standardize\_columns), .SDcols }\OtherTok{=}\NormalTok{ input\_columns]}

\CommentTok{\# Calculate average churn probabilities in training and validation samples}
\NormalTok{avg\_churn\_train }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(DT\_lin\_prob}\SpecialCharTok{$}\NormalTok{churn)}
\NormalTok{avg\_churn\_valid }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(Dv\_lin\_prob}\SpecialCharTok{$}\NormalTok{churn)}

\CommentTok{\# Specify the formula excluding the first three columns}
\NormalTok{independent\_vars }\OtherTok{=} \FunctionTok{names}\NormalTok{(DT\_lin\_prob)[}\SpecialCharTok{{-}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)]}
\NormalTok{formula }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"churn \textasciitilde{}"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(independent\_vars, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{)))}

\CommentTok{\# Fit the lpm model}
\NormalTok{lm\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ cell2cell\_clean, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
 extra argument 'family' will be disregarded
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tidy the linear probability model output}
\NormalTok{tidy\_model }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(lm\_model)}

\CommentTok{\# Add effect\_size column}
\NormalTok{tidy\_model }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{effect\_size =}\NormalTok{ estimate }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{100} \SpecialCharTok{*}\NormalTok{ avg\_churn\_valid }\SpecialCharTok{/}\NormalTok{ avg\_churn\_train))}

\CommentTok{\# Sort by absolute effect\_size and print the table}
\NormalTok{tidy\_model }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(}\FunctionTok{abs}\NormalTok{(effect\_size)))}
\FunctionTok{kable}\NormalTok{(tidy\_model)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1791}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1642}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1493}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1791}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1493}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1791}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
std.error
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
effect\_size
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(Intercept) & 0.3155318 & 0.0154054 & 20.4819562 & 0.0000000 &
1.2222911 \\
retcall & 0.1428622 & 0.0334030 & 4.2769308 & 0.0000190 & 0.5534125 \\
creditaa & -0.0746562 & 0.0054187 & -13.7775804 & 0.0000000 &
-0.2891994 \\
refurb & 0.0497153 & 0.0052670 & 9.4389822 & 0.0000000 & 0.1925846 \\
retaccpt & -0.0454461 & 0.0180450 & -2.5184894 & 0.0117882 &
-0.1760470 \\
retcalls & 0.0436851 & 0.0318359 & 1.3721972 & 0.1700065 & 0.1692253 \\
credita & -0.0374406 & 0.0057344 & -6.5290790 & 0.0000000 &
-0.1450354 \\
webcap & -0.0325533 & 0.0062916 & -5.1741180 & 0.0000002 & -0.1261035 \\
actvsubs & -0.0260920 & 0.0042041 & -6.2063671 & 0.0000000 &
-0.1010739 \\
occhmkr & 0.0255888 & 0.0300500 & 0.8515430 & 0.3944707 & 0.0991247 \\
prizmrur & 0.0251413 & 0.0082316 & 3.0542525 & 0.0022571 & 0.0973913 \\
children & 0.0219170 & 0.0045942 & 4.7705388 & 0.0000018 & 0.0849008 \\
uniqsubs & 0.0216580 & 0.0023639 & 9.1619086 & 0.0000000 & 0.0838977 \\
marryun & 0.0184553 & 0.0055491 & 3.3258146 & 0.0008821 & 0.0714911 \\
mailres & -0.0182875 & 0.0139969 & -1.3065363 & 0.1913746 &
-0.0708411 \\
mcycle & 0.0179938 & 0.0147568 & 1.2193553 & 0.2227135 & 0.0697033 \\
setprcm & -0.0179207 & 0.0065143 & -2.7509916 & 0.0059431 &
-0.0694203 \\
refer & -0.0164161 & 0.0065690 & -2.4990389 & 0.0124554 & -0.0635920 \\
creditcd & 0.0162241 & 0.0069966 & 2.3188679 & 0.0204051 & 0.0628480 \\
occcler & 0.0159186 & 0.0122084 & 1.3038995 & 0.1922722 & 0.0616646 \\
occstud & 0.0150096 & 0.0195761 & 0.7667340 & 0.4432423 & 0.0581436 \\
incmiss & -0.0137615 & 0.0097467 & -1.4119094 & 0.1579811 &
-0.0533085 \\
marryyes & 0.0109617 & 0.0052656 & 2.0817593 & 0.0373681 & 0.0424629 \\
occret & -0.0107438 & 0.0146348 & -0.7341284 & 0.4628730 & -0.0416190 \\
prizmtwn & 0.0099962 & 0.0051531 & 1.9398633 & 0.0524004 & 0.0387229 \\
phones & 0.0091587 & 0.0028875 & 3.1718725 & 0.0015153 & 0.0354785 \\
mailord & -0.0086711 & 0.0139311 & -0.6224300 & 0.5336612 &
-0.0335897 \\
ownrent & 0.0080209 & 0.0069890 & 1.1476508 & 0.2511167 & 0.0310710 \\
newcelly & -0.0068243 & 0.0044463 & -1.5348272 & 0.1248309 &
-0.0264356 \\
newcelln & 0.0067779 & 0.0051506 & 1.3159517 & 0.1881946 & 0.0262560 \\
prizmub & -0.0065886 & 0.0039796 & -1.6555940 & 0.0978086 &
-0.0255224 \\
truck & 0.0061767 & 0.0058655 & 1.0530690 & 0.2923130 & 0.0239271 \\
occcrft & -0.0058655 & 0.0102090 & -0.5745377 & 0.5656058 &
-0.0227213 \\
threeway & -0.0056046 & 0.0015879 & -3.5296127 & 0.0004164 &
-0.0217107 \\
pcown & 0.0046355 & 0.0050451 & 0.9188126 & 0.3581968 & 0.0179568 \\
months & -0.0039985 & 0.0003112 & -12.8490599 & 0.0000000 &
-0.0154893 \\
rv & 0.0032359 & 0.0078461 & 0.4124217 & 0.6800316 & 0.0125350 \\
models & 0.0030603 & 0.0044645 & 0.6854850 & 0.4930402 & 0.0118550 \\
occprof & -0.0028973 & 0.0052951 & -0.5471700 & 0.5842637 &
-0.0112236 \\
callfwdv & -0.0023206 & 0.0030577 & -0.7589262 & 0.4478993 &
-0.0089893 \\
occself & -0.0022278 & 0.0131317 & -0.1696487 & 0.8652869 &
-0.0086298 \\
mailflag & 0.0021229 & 0.0141718 & 0.1497977 & 0.8809247 & 0.0082236 \\
dropvce & 0.0015669 & 0.0013081 & 1.1978461 & 0.2309810 & 0.0060696 \\
income & -0.0013005 & 0.0009818 & -1.3246965 & 0.1852762 & -0.0050380 \\
travel & 0.0011499 & 0.0077021 & 0.1493014 & 0.8813163 & 0.0044545 \\
custcare & -0.0009738 & 0.0003752 & -2.5958113 & 0.0094388 &
-0.0037724 \\
age1 & -0.0008056 & 0.0001416 & -5.6910243 & 0.0000000 & -0.0031208 \\
blckvce & 0.0006473 & 0.0012956 & 0.4995951 & 0.6173618 & 0.0025074 \\
recchrge & -0.0005385 & 0.0001443 & -3.7308807 & 0.0001910 &
-0.0020861 \\
roam & 0.0005263 & 0.0002299 & 2.2892024 & 0.0220706 & 0.0020389 \\
changer & 0.0005231 & 0.0000555 & 9.4178059 & 0.0000000 & 0.0020265 \\
revenue & 0.0003348 & 0.0001279 & 2.6177138 & 0.0088540 & 0.0012970 \\
eqpdays & 0.0002750 & 0.0000118 & 23.3543418 & 0.0000000 & 0.0010653 \\
incalls & -0.0002686 & 0.0001654 & -1.6240220 & 0.1043757 &
-0.0010404 \\
callwait & 0.0001829 & 0.0004725 & 0.3871468 & 0.6986487 & 0.0007086 \\
age2 & -0.0001770 & 0.0001100 & -1.6091271 & 0.1075931 & -0.0006856 \\
overage & 0.0001579 & 0.0000451 & 3.4999792 & 0.0004656 & 0.0006116 \\
unansvce & 0.0001527 & 0.0000705 & 2.1647194 & 0.0304126 & 0.0005914 \\
peakvce & -0.0001153 & 0.0000345 & -3.3417639 & 0.0008329 &
-0.0004468 \\
outcalls & 0.0001038 & 0.0000917 & 1.1319893 & 0.2576429 & 0.0004021 \\
changem & -0.0001028 & 0.0000085 & -12.0987909 & 0.0000000 &
-0.0003981 \\
setprc & 0.0000899 & 0.0000454 & 1.9797035 & 0.0477408 & 0.0003482 \\
mou & -0.0000508 & 0.0000080 & -6.3614191 & 0.0000000 & -0.0001970 \\
opeakvce & -0.0000471 & 0.0000416 & -1.1318473 & 0.2577025 &
-0.0001824 \\
dropblk & -0.0000304 & 0.0012767 & -0.0238423 & 0.9809785 &
-0.0001179 \\
mourec & 0.0000274 & 0.0000212 & 1.2945619 & 0.1954757 & 0.0001061 \\
directas & 0.0000122 & 0.0008945 & 0.0136384 & 0.9891185 & 0.0000473 \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  In order to create a table that captures the linear probability model
  estimates, use the \texttt{tidy} function. Add a column, e.g.
  \texttt{effect\_size}, that scales the estimates by the factor
  \[100 \cdot \frac{\bar{p}_{v}}{\bar{p}_{t}}\] This scales the effect
  sizes to the correct magnitude of the churn probabilities in the
  validation sample and puts the effects on a 0-100\% scale. Sort the
  variables according to the magnitude of the effect sizes, and print
  the results table using \texttt{kable}.
\item
  Inspect the results. Identify some factors that are strongly
  associated with churn. If actionable, propose an incentive that can be
  targeted to the customers to prevent churn.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sorted\_model }\OtherTok{\textless{}{-}}\NormalTok{ tidy\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(effect\_size))}
\NormalTok{top\_5 }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(sorted\_model, }\DecValTok{5}\NormalTok{)}
\NormalTok{bottom\_5 }\OtherTok{\textless{}{-}} \FunctionTok{tail}\NormalTok{(sorted\_model, }\DecValTok{5}\NormalTok{)}

\FunctionTok{print}\NormalTok{(top\_5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 6
  term        estimate std.error statistic  p.value effect_size
  <chr>          <dbl>     <dbl>     <dbl>    <dbl>       <dbl>
1 (Intercept)   0.316    0.0154     20.5   5.89e-93      1.22  
2 retcall       0.143    0.0334      4.28  1.90e- 5      0.553 
3 refurb        0.0497   0.00527     9.44  3.88e-21      0.193 
4 retcalls      0.0437   0.0318      1.37  1.70e- 1      0.169 
5 occhmkr       0.0256   0.0300      0.852 3.94e- 1      0.0991
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(bottom\_5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 6
  term     estimate std.error statistic  p.value effect_size
  <chr>       <dbl>     <dbl>     <dbl>    <dbl>       <dbl>
1 actvsubs  -0.0261   0.00420     -6.21 5.45e-10      -0.101
2 webcap    -0.0326   0.00629     -5.17 2.30e- 7      -0.126
3 credita   -0.0374   0.00573     -6.53 6.66e-11      -0.145
4 retaccpt  -0.0454   0.0180      -2.52 1.18e- 2      -0.176
5 creditaa  -0.0747   0.00542    -13.8  3.96e-43      -0.289
\end{verbatim}

Factors that are strongly associated with churn.

\textbf{Strong positively churn}

Assuming p\textless0.05 is statistically significant

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  retcall (0.55341249) : Customers with retention calls are more likely
  to churn, possibly due to dissatisfaction with previous interactions
  or unresolved issues. Highly significant (p=1.89e-05)

  Incentive :

  \begin{itemize}
  \tightlist
  \item
    Personalized Service Recovery: Provide dedicated, high-quality
    customer service follow-ups for customers who have made a retention
    call, ensuring their issues are fully resolved. - Offer discounts or
    credits (e.g., one-time \$50 credit) for the inconvenience.
  \end{itemize}
\item
  refurb (0.19258463) : Customers with refurbished devices are more
  likely to churn, possibly due to dissatisfaction with older hardware.
  Highly significant(p=3.87e10-21)

  Incentive :

  \begin{itemize}
  \tightlist
  \item
    Upgrade Program : Create trade-in programs allowing refurbished
    users to easily swap for newer models at minimal cost.
  \end{itemize}
\end{enumerate}

Note : retcalls frequency (0.16922533) and occhmkr (0.09912475) has high
magnitude but not statistically significant p \textgreater{} 0.05.

\textbf{Strong negatively churn (unlikely to churn)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  creditaa (-0.2891994) : Customers with excellent credit ratings `aa'
  are less likely to churn, possibly indicating higher satisfaction or
  loyalty.

  Incentive :

  \begin{itemize}
  \tightlist
  \item
    Offer loyalty programs, such as discounts on premium services or
    free upgrades, for customers who meet certain payment and credit
    score thresholds.
  \end{itemize}
\item
  retaccpt (-0.1760470) : Customers who accept retention offers are less
  likely to churn, highlighting the effectiveness of these
  interventions.

  Incentive :

  \begin{itemize}
  \tightlist
  \item
    Post retention engagement : Provide incentives for continued
    loyalty, such as bonus points or exclusive access to new features.
  \end{itemize}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  credita (-0.1450354) : Customers with excellent credit ratings `a' are
  less likely to churn. Similar to creditaa.

  Incentive :

  \begin{itemize}
  \tightlist
  \item
    Send message to encourage them to earn more points to move to the
    `aa' tier.
  \end{itemize}
\end{enumerate}

\newpage

\section{Economics of churn
management}\label{economics-of-churn-management}

Next, we would like to predict the value of a proposed churn management
program in order to assess the maximum amount that we would spend to
prevent a customer from churning for one year.

\emph{Perform this prediction}, under the following assumptions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We consider a planning horizon of 4 years (the current year and three
  additional years), and an annual discount rate of 8 percent.
\item
  Predict the churn management value for 20 groups, but keep in mind
  that it is good practice to make sure the code works for an arbitrary
  number of groups in case we wish to modify that in the future. Predict
  the program value for these 20 customer segments based on the
  predicted churn rate. Note that we create these segments based on the
  validation sample data. We predict current and future customer profits
  at the year-level. Hence, we also need to convert both the monthly
  churn rate and the revenue data to the year-level.
\item
  Assume that the churn management program has a success probability
  \texttt{gamma} (\(\gamma\)) and compare the results for
  \(\gamma=0.25\) and \(\gamma=0.5\).
\end{enumerate}

\medskip

Hint: It is easy to make little mistakes in the lifetime value
predictions. Hence, be very clear about what your code is supposed to
achieve, and check that every step is correct.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Program Value }
\CommentTok{\# Gamma = 0.25 : Success probability 25\%. Churn with Incentive is 75\%.}
\CommentTok{\# Gamma = 0.50 : Success probability 50\%. Churn with Incentive is 50\%.}

\CommentTok{\# Parameters}
\NormalTok{discount\_rate }\OtherTok{\textless{}{-}} \FloatTok{0.08}
\NormalTok{gamma\_values }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{groups }\OtherTok{\textless{}{-}} \DecValTok{20}
\NormalTok{horizon }\OtherTok{\textless{}{-}} \DecValTok{4}  \CommentTok{\# 4 years}

\CommentTok{\# Convert monthly churn rate and revenue to yearly}
\NormalTok{validation\_data }\OtherTok{\textless{}{-}}\NormalTok{ validation\_sample }
\NormalTok{validation\_data[, churn\_rate\_yearly }\SpecialCharTok{:=} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ predicted\_churn)}\SpecialCharTok{\^{}}\DecValTok{12}\NormalTok{]}
\NormalTok{validation\_data[, revenue\_yearly }\SpecialCharTok{:=}\NormalTok{ revenue }\SpecialCharTok{*} \DecValTok{12}\NormalTok{]}

\CommentTok{\# Group data into segments }
\CommentTok{\# The segment is in the score\_group column}
\NormalTok{validation\_data[, score\_group }\SpecialCharTok{:=} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{cut\_number}\NormalTok{(predicted\_churn, }\AttributeTok{n =}\NormalTok{ groups))]}

\CommentTok{\# Function to calculate LTV}
\NormalTok{calculate\_ltv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(revenue, churn\_rate, discount\_rate, horizon) \{}
\NormalTok{  ltv }\OtherTok{\textless{}{-}} \DecValTok{0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{0}\SpecialCharTok{:}\NormalTok{(horizon }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) \{}
\NormalTok{    ltv }\OtherTok{\textless{}{-}}\NormalTok{ ltv }\SpecialCharTok{+}\NormalTok{ (revenue }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ churn\_rate)}\SpecialCharTok{\^{}}\NormalTok{t) }\SpecialCharTok{/}\NormalTok{ ((}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ discount\_rate)}\SpecialCharTok{\^{}}\NormalTok{t)}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(ltv)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate metrics for each group}
\NormalTok{group\_summary }\OtherTok{\textless{}{-}}\NormalTok{ validation\_data[, .(}
  \AttributeTok{avg\_churn\_rate =} \FunctionTok{mean}\NormalTok{(churn\_rate\_yearly),}
  \AttributeTok{avg\_revenue =} \FunctionTok{mean}\NormalTok{(revenue\_yearly),}
  \AttributeTok{avg\_ltv =} \FunctionTok{calculate\_ltv}\NormalTok{(}\FunctionTok{mean}\NormalTok{(revenue\_yearly), }\FunctionTok{mean}\NormalTok{(churn\_rate\_yearly), }
\NormalTok{                          discount\_rate, horizon)}
\NormalTok{  ), by }\OtherTok{=}\NormalTok{ score\_group]}

\CommentTok{\# Churn Rate with incentive}
\NormalTok{group\_summary[, churn\_0}\FloatTok{.25} \SpecialCharTok{:=}\NormalTok{ (}\DecValTok{1}\FloatTok{{-}0.25}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ avg\_churn\_rate]}
\NormalTok{group\_summary[, churn\_0}\FloatTok{.5} \SpecialCharTok{:=}\NormalTok{ (}\DecValTok{1}\FloatTok{{-}0.5}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ avg\_churn\_rate]}

\NormalTok{group\_summary[, ltv\_0}\FloatTok{.25} \SpecialCharTok{:=} \FunctionTok{calculate\_ltv}\NormalTok{(avg\_revenue, churn\_0}\FloatTok{.25}\NormalTok{, discount\_rate, horizon)]}
\NormalTok{group\_summary[, ltv\_0}\FloatTok{.5} \SpecialCharTok{:=} \FunctionTok{calculate\_ltv}\NormalTok{(avg\_revenue, churn\_0}\FloatTok{.5}\NormalTok{, discount\_rate, horizon)]}

\CommentTok{\# WTP}
\NormalTok{group\_summary[, WTP\_0}\FloatTok{.25} \SpecialCharTok{:=}\NormalTok{  ltv\_0}\FloatTok{.25} \SpecialCharTok{{-}}\NormalTok{ avg\_ltv]}
\NormalTok{group\_summary[, WTP\_0}\FloatTok{.5} \SpecialCharTok{:=}\NormalTok{ ltv\_0}\FloatTok{.5} \SpecialCharTok{{-}}\NormalTok{ avg\_ltv]}

\CommentTok{\# Profitability if cost is $100}
\NormalTok{group\_summary[, Profit\_g0}\FloatTok{.25}\NormalTok{\_c100 }\SpecialCharTok{:=}\NormalTok{ WTP\_0}\FloatTok{.25} \SpecialCharTok{{-}} \DecValTok{100}\NormalTok{]}
\NormalTok{group\_summary[, Profit\_g0}\FloatTok{.5}\NormalTok{\_c100 }\SpecialCharTok{:=}\NormalTok{ WTP\_0}\FloatTok{.5} \SpecialCharTok{{-}} \DecValTok{100}\NormalTok{]}

\CommentTok{\# Profitability if cost is $175}
\NormalTok{group\_summary[, Profit\_g0}\FloatTok{.25}\NormalTok{\_c175 }\SpecialCharTok{:=}\NormalTok{ WTP\_0}\FloatTok{.25} \SpecialCharTok{{-}} \DecValTok{175}\NormalTok{]}
\NormalTok{group\_summary[, Profit\_g0}\FloatTok{.5}\NormalTok{\_c175 }\SpecialCharTok{:=}\NormalTok{ WTP\_0}\FloatTok{.5} \SpecialCharTok{{-}} \DecValTok{175}\NormalTok{]}

\NormalTok{group\_summary }\OtherTok{\textless{}{-}}\NormalTok{ group\_summary[}\FunctionTok{order}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{WTP\_0}\FloatTok{.5}\NormalTok{)]}

\CommentTok{\# View the updated table}
\FunctionTok{kable}\NormalTok{(group\_summary[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{], }\AttributeTok{digits =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{font\_size =} \DecValTok{8}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begingroup\fontsize{8}{10}\selectfont

\begin{longtable}[t]{rrrrrrrrr}
\toprule
score\_group & avg\_churn\_rate & avg\_revenue & avg\_ltv & churn\_0.25 & churn\_0.5 & ltv\_0.25 & ltv\_0.5 & WTP\_0.25\\
\midrule
20 & 0.42 & 856.16 & 1695.42 & 0.32 & 0.21 & 1962.00 & 2275.57 & 266.58\\
19 & 0.32 & 696.42 & 1586.37 & 0.24 & 0.16 & 1775.35 & 1988.02 & 188.98\\
18 & 0.29 & 637.67 & 1521.07 & 0.21 & 0.14 & 1683.17 & 1863.12 & 162.10\\
17 & 0.27 & 625.23 & 1537.26 & 0.20 & 0.13 & 1688.43 & 1854.79 & 151.17\\
16 & 0.25 & 645.78 & 1625.52 & 0.19 & 0.12 & 1775.02 & 1938.44 & 149.50\\
\addlinespace
15 & 0.24 & 648.65 & 1662.94 & 0.18 & 0.12 & 1807.63 & 1965.00 & 144.70\\
14 & 0.22 & 640.54 & 1667.42 & 0.17 & 0.11 & 1805.64 & 1955.32 & 138.21\\
13 & 0.21 & 623.76 & 1646.35 & 0.16 & 0.11 & 1776.69 & 1917.32 & 130.34\\
11 & 0.20 & 664.12 & 1797.23 & 0.15 & 0.10 & 1927.46 & 2067.01 & 130.23\\
12 & 0.21 & 626.87 & 1675.94 & 0.15 & 0.10 & 1802.84 & 1939.28 & 126.90\\
\addlinespace
10 & 0.19 & 637.45 & 1744.94 & 0.14 & 0.09 & 1866.03 & 1995.39 & 121.08\\
8 & 0.17 & 660.22 & 1847.54 & 0.13 & 0.09 & 1964.88 & 2089.51 & 117.34\\
7 & 0.17 & 685.86 & 1941.09 & 0.12 & 0.08 & 2058.54 & 2182.91 & 117.45\\
9 & 0.18 & 636.39 & 1761.43 & 0.14 & 0.09 & 1878.45 & 2003.10 & 117.02\\
5 & 0.15 & 731.57 & 2122.20 & 0.11 & 0.07 & 2236.72 & 2357.22 & 114.52\\
\addlinespace
6 & 0.16 & 690.73 & 1977.92 & 0.12 & 0.08 & 2091.44 & 2211.29 & 113.52\\
4 & 0.14 & 764.15 & 2249.57 & 0.10 & 0.07 & 2362.23 & 2480.33 & 112.66\\
3 & 0.13 & 821.25 & 2460.57 & 0.09 & 0.06 & 2572.42 & 2689.14 & 111.85\\
2 & 0.11 & 845.28 & 2585.54 & 0.08 & 0.06 & 2689.05 & 2796.52 & 103.51\\
1 & 0.08 & 1033.05 & 3279.97 & 0.06 & 0.04 & 3379.49 & 3481.85 & 99.52\\
\bottomrule
\end{longtable}
\endgroup{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(group\_summary[, }\DecValTok{10}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(group\_summary)], }\AttributeTok{digits =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable\_styling}\NormalTok{(}\AttributeTok{font\_size =} \DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begingroup\fontsize{8}{10}\selectfont

\begin{longtable}[t]{rrrrr}
\toprule
WTP\_0.5 & Profit\_g0.25\_c100 & Profit\_g0.5\_c100 & Profit\_g0.25\_c175 & Profit\_g0.5\_c175\\
\midrule
580.15 & 166.58 & 480.15 & 91.58 & 405.15\\
401.65 & 88.98 & 301.65 & 13.98 & 226.65\\
342.05 & 62.10 & 242.05 & -12.90 & 167.05\\
317.53 & 51.17 & 217.53 & -23.83 & 142.53\\
312.92 & 49.50 & 212.92 & -25.50 & 137.92\\
\addlinespace
302.07 & 44.70 & 202.07 & -30.30 & 127.07\\
287.90 & 38.21 & 187.90 & -36.79 & 112.90\\
270.96 & 30.34 & 170.96 & -44.66 & 95.96\\
269.78 & 30.23 & 169.78 & -44.77 & 94.78\\
263.34 & 26.90 & 163.34 & -48.10 & 88.34\\
\addlinespace
250.44 & 21.08 & 150.44 & -53.92 & 75.44\\
241.96 & 17.34 & 141.96 & -57.66 & 66.96\\
241.82 & 17.45 & 141.82 & -57.55 & 66.82\\
241.67 & 17.02 & 141.67 & -57.98 & 66.67\\
235.03 & 14.52 & 135.03 & -60.48 & 60.03\\
\addlinespace
233.37 & 13.52 & 133.37 & -61.48 & 58.37\\
230.76 & 12.66 & 130.76 & -62.34 & 55.76\\
228.57 & 11.85 & 128.57 & -63.15 & 53.57\\
210.98 & 3.51 & 110.98 & -71.49 & 35.98\\
201.88 & -0.48 & 101.88 & -75.48 & 26.88\\
\bottomrule
\end{longtable}
\endgroup{}

Analyzing Result :

Value of a proposed churn management program (WTP) Per segment

\begin{itemize}
\item
  WTP\_0.25 is the value of a proposed churn management program that has
  0.25 success value.
\item
  For example, for score group 20, we can spend up to \$105.96373 per
  customer per 4 year, assuming a 25\% success probability (𝛾= 0.25 ).
\item
  WTP\_0.5 is the value of a proposed churn management program that has
  0.5 success value.
\item
  For example, for score group 20, we can spend up to \$580.1549 per
  customer per 4 year, assuming a 50\% success probability (𝛾= 0.5 ).
\end{itemize}

Maximum amount that we would spend to prevent a customer from churning
for one year.

\begin{itemize}
\item
  Max WTP per 1 year, is the WTP after applying the program for Y0
  -\textgreater{} Y1. After then the churn will go from 0
  -\textgreater{} 1 (retention). Even if the program makes customer
  stays as long as 4 years. We'll only consider that we spent \$100
  program cost on the first year.
\item
  So the Max WTP per 1 year is WTP\_0.25 and WTP\_0.5
\end{itemize}

\newpage

\section{Summarize your main results}\label{summarize-your-main-results}

\emph{Please organize your main results along the four questions posed
in the overview.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Is customer churn at Cell2Cell predictable from the customer
  information that Cell2Cell maintains?

  Yes, customer churn at Cell2Cell is predictable using the customer
  data provided.

  \begin{itemize}
  \item
    Predictive Performance: The logistic regression model successfully
    predicts churn with close alignment between the average predicted
    churn rate (0.0194) and the observed churn rate (0.0193) in the
    validation sample after adjusting for oversampling.
  \item
    Lift Analysis: The model demonstrates strong predictive power
    through lift analysis, where higher score groups exhibit
    significantly higher churn rates compared to lower groups,
    confirming the model's ability to rank customers by churn risk.
    Additionally, when applied the baseline churn rate, It hits close to
    the mid segment group reflects that the churn `predictions' align
    with the `observation' of churn -- the reality that is expected per
    each segment.
  \end{itemize}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  What factors drive customer churn? Which factors are particularly
  important?

  See also part 6 analysis,

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    retcall (0.55341249) : Customers with retention calls are more
    likely to churn, possibly due to dissatisfaction with previous
    interactions or unresolved issues. Highly significant
    (effect\_size=0.55341249 , p=1.89e-05)
  \item
    creditaa (-0.2891994) : Customers with excellent credit ratings `aa'
    are less likely to churn, possibly indicating higher satisfaction or
    loyalty. Highly significant (effect\_size=-0.2891994 , p=3.96e-43)
  \item
    refurb (0.19258463) : Customers with refurbished devices are more
    likely to churn, possibly due to dissatisfaction with older
    hardware. Highly significant (effect\_size=0.19258463, p=3.88e-21)
  \end{enumerate}
\item
  What incentives should Cell2Cell offer to prevent customer churn?
\end{enumerate}

See also part 6 analysis,

\begin{itemize}
\item
  Personalized Service Recovery: Provide dedicated, high-quality
  customer service follow-ups for customers who have made a retention
  call, ensuring their issues are fully resolved. - Offer discounts or
  credits (e.g., one-time \$50 credit) for the inconvenience.
\item
  Implement a proactive monitoring system to identify frequent retention
  team callers and address their issues before they escalate.
\item
  Provide special discounts or perks (e.g., reduced fees for 3 months)
  to frequent retention callers to demonstrate appreciation and regain
  trust.
\item
  Upgrade Program : Create trade-in programs allowing refurbished users
  to easily swap for newer models at minimal cost.
\item
  Offer loyalty programs, such as discounts on premium services or free
  upgrades, for customers who meet certain payment and credit score
  thresholds. This will incentivize `aa' credit customer to stay loyal.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  What is the economic value of a proposed targeted plan to prevent
  churn, and how does this value differ across customer segments?
  Compare the economic value to an incentive with a cost of \$100 and
  another incentive with a cost of \$175. Which customers segments
  should receive the incentive? Does the answer depend on the success
  probability?

  \begin{itemize}
  \tightlist
  \item
    Customer in segment 20 (high churn rate) provides higher benefit
    from churn management program than customer in segment 1 (low churn
    rate). This is because if we apply the program to the group that is
    likely to churn, we can gain the benefit from the customer that
    continue to stay with us.
  \item
    Cost of the incentive is \$100,

    \begin{itemize}
    \tightlist
    \item
      If the program has 0.25 success rate, Cell2Cell will earn profit
      if apply program to segment 2-19. The segment 1 shown
      -\$0.4843072, suggesting that the program cost is higher than the
      value that churn program will give. Hence, it is not worth
      investing in segment 1.
    \item
      If the program has 0.5 success rate, Cell2Cell can earn profit
      from all segments.
    \end{itemize}
  \item
    Cost of the incentive is \$175,

    \begin{itemize}
    \tightlist
    \item
      If the program has 0.25 success rate, Cell2Cell should target only
      19-20th segments because they offer profit.
    \item
      If the program has 0.5 success rate, Cell2Cell can target all
      segments.
    \end{itemize}
  \item
    To be the most effective, Cell2Cell should target the customer with
    higher churn rate like 20th segments if the likelihood of program
    success is low (0.25) and program cost is between \$100-175.
    However, if the likelihood of program success is high (0.50) it can
    target any segments given the cost of implementing program is
    between \$100-175.
  \end{itemize}
\end{enumerate}

\end{document}
