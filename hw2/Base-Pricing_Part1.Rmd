---
title: "Base Pricing Analysis and Price Elasticity Estimation"
author: "Giovanni Compiani"
output:
  pdf_document:
    number_sections: yes
    toc: yes
header-includes: \usepackage{booktabs}
graphics: yes
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, eval = TRUE,
                      fig.width = 4.5, fig.height = 3, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage

# Overview

The goal is to conduct a base pricing analysis. We estimate brand-level demand using scanner data, and then we make profitability predictions corresponding to specific base price changes. We estimate log-linear demand models that use (log) prices and promotions as inputs, and predict log quantities, `log(1+Q)`. The models predict the demand for a focal brand, and we control for (log) prices and promotions of three competitors. Obviously, this approach generalizes to an arbitrarily large number of competing products as long as the sample size is large enough.

Our focus is on the two top brands in the liquid laundry detergent category, *Tide* and *Gain*. Both are Procter & Gamble brands. The two closest competitors are *Arm & Hammer* and *Purex*.

# Packages

Make sure to install two packages that we have not used before: fixest and knitr.

```{r}
library(bit64)
library(data.table)
library(fixest)
library(knitr)
library(ggplot2)
```

\newpage

# Data overview

The data are located in this folder:

```{r}

data_folder = "/Users/nichada/MyCode/MPCS/_BUSN_DataSci/DataSci_Mkt/hw2/data"
#data_folder = "Data"
```

\bigskip

The data source is an extract from the Nielsen RMS retail scanner data set. The data set captures weekly price and quantity data for all products (UPC's) sold in the stores of a large number of U.S. retail chains. The Kilts data do not include all retailers (for example, Walmart is not part of the data), and the identity of the retailers is not revealed. However, we know if two stores belong to the same retail chain.

## Brand data

The data.table `brands` in `Brands.RData` includes brand information for the top five brands in three categories (product modules):

```         
1036   FRUIT JUICE - LEMON/LIME
1040   FRUIT JUICE - ORANGE - OTHER CONTAINER
7012   DETERGENTS - HEAVY DUTY - LIQUID
```

The data include the brand code, brand description, and total revenue calculated across all observations. The top five brands were selected based on total brand revenue.

We will focus on the liquid laundry detergent category with corresponding `product_module_code` 7012.

## Store data

Inspect the table `stores` in the file `Stores.RData`. The variable `store_code_uc` identifies each retail stores. For some (but not all) stores we know the corresponding `retailer_code` that identifies the chain (banner) that the store belongs to. The data include the Scantrack (SMM) market code and the Scantrack market description. Scantrack markets correspond to large metropolitan market areas such as *Chicago* or *Raleigh-Durham* (see the data manual for a map of the Scantrack markets). The three-digit ZIP code of each store is also included.

## Movement data

The movement data (`move`) are in files of the form `brand_move_<module code>.RData`. The data are at the brand/store/week level and include prices and quantities (`units`). The data are aggregates of all UPC's that share the same brand name. Brand prices are measured as the weighted average over all store/week UPC prices in equivalent units, and quantities represent total product volume measured in equivalent units such as ounces. In the liquid laundry detergent category (module 7012), prices represent dollars per ounce and units are total product volume in ounces per store/week. The aggregation weights are based on total store-level UPC revenue across all weeks, and hence the aggregation weights are constant within each store. The movement data also include a promotion indicator (`promo_dummy`), a logical `TRUE/FALSE` variable.

The `week_end` variable date is the last day of a Nielsen week, which always starts on a Sunday and ends on a Saturday. Note that prices may change during the period, and hence even the UPC-level price may be an average over more than one posted price. The sample includes data for the 2010-2013 period.

Please consult the official Kilts Center Retail Scanner Dataset Manual for all details.

\newpage

# Prepare the data for the demand analysis

We first load the brand and store data.

```{r}
load(paste0(data_folder, "/Brands.RData"))
load(paste0(data_folder, "/Stores.RData"))
```

## Select the category and brands

\*Choose the laundry detergent category (module) and select the corresponding brand-level meta data from the data table `brands`. Then sort (order) the brand data corresponding to total brand revenue, and select the **top four brands** (ranked by revenue).

```{r}
selected_module = 7012                 # Laundry detergent
laundry_brands = brands[product_module_code == selected_module]

sorted_brands = laundry_brands[order(-laundry_brands$revenue), ]

top_four_brands = head(sorted_brands, 4)

top_four_brands
```

\medskip

*Let's assign each brand a new name using a new variable, `brand_name`, and give the four brands simple names such as `Tide`, `Gain`, `ArmHammer`, and `Purex`. These simplified brand names will make our code and the estimation output more readable.* More specifically, create a new data containing the four selected brands and add to it the `brand_name` variable.

Note that we will add the brand names to the quantity, price, and promotion variables. In R, `price_ArmHammer` (as well as `price_Arm_Hammer`) are legal variable names, but `price_Arm&Hammer` and `price_Arm & Hammer` are not, and hence I do not suggest the brand names `Arm&Hammer` or `Arm & Hammer`.

```{r}
top_four_brands$brand_name = NA 

top_four_brands$brand_name[top_four_brands$brand_descr == "TIDE - H-D LIQ"] = "Tide"
top_four_brands$brand_name[top_four_brands$brand_descr == "GAIN - H-D LIQ"] = "Gain"
top_four_brands$brand_name[top_four_brands$brand_descr == "ARM & HAMMER - H-D LIQ"] = "ArmHammer"
top_four_brands$brand_name[top_four_brands$brand_descr == "PUREX - H-D LIQ"] = "Purex"

top_four_brands

```

## Prepare the movement data

\*Load the movement data, and---for better readability---change the variable names from `units` to `quantity` and from `promo_dummy` to `promotion` (you can use the `setnames` command for this). Change the data type of the `promotion` variable from `logical` to `numeric` using the `as.numeric` function. Finally, merge the new `brand_name` variable with the movement table (more precisely, perform an inner join, i.e. retain all observations that are present in both the parent and child data sets).

```{r}
load(paste0(data_folder, "/brand_move_7012.RData"))
setnames(move, old = c("units", "promo_dummy"), new = c("quantity", "promotion"))
move$promotion = as.numeric(move$promotion)
merged_data = merge(move, top_four_brands[,c("brand_code_uc","brand_name")], by = "brand_code_uc")
merged_data
```

## Remove outliers

Most data contain some "flaws" or outliers. Here is an easy way of removing such outliers:

First, we create a function that flags all observations in a vector `x`, for example a price series, as outliers if the ratio between a value and the median value among all `x` observations is below or above a threshold.

```{r}
isOutlier <- function(x, threshold_bottom, threshold_top) {
   is_outlier = rep(FALSE, times = length(x))
   median_x   = median(x, na.rm = TRUE)
   is_outlier[x/median_x < threshold_bottom | x/median_x > threshold_top] = TRUE
   return(is_outlier)
}
```

*Now run this function on the price data, separately for each brand and store. Then tabulate the number of outliers, and remove the corresponding observations from the data set.*

I recommend to use a lower threshold (`threshold_bottom`) value of 0.35 and an upper threshold (`threshold_top`) of 2.5.

```{r}
threshold_bottom = 0.35
threshold_top = 2.5

## not sure if needed:
## merged_data_means = merged_data[, lapply(.SD, mean), 
##                                 by = .(brand_code_uc, store_code_uc), 
##                                 .SDcols = c("price")]

merged_data[,isoutlier:=isOutlier(price,threshold_bottom,threshold_top), by = .(brand_code_uc, store_code_uc)]

# tabulate data
outlier_counts <- merged_data[isoutlier == TRUE, .(outlier_count = .N), by = .(brand_code_uc, brand_name, store_code_uc)]
print(outlier_counts)
```

```{r}
merged_data_cleaned = merged_data[isoutlier=="FALSE"]

merged_data_cleaned
```

## Reshape the movement data from long to wide format

To prepare the data for the regression analysis, we need to **reshape the data from long to wide format** using **`dcast`**.

All the details on casting and the reverse operation (melting from wide to long format using `melt`) are explained in the data.table html vignettes:

<https://rdatatable.gitlab.io/data.table/articles/datatable-reshape.html>

Let's be specific about the structure of the data that we need to use to estimate a demand model. We would like to obtain a table with observations, characterized by a combination of store id (`store_code_uc`) and week (`week_end`) in rows, and information on quantities, prices, and promotions in columns. Quantities, prices, and promotions are brand-specific.

```{r}
# Step 1: Melt the data into long format
melted_data <- melt(merged_data_cleaned, 
                    id.vars = c("store_code_uc", "week_end", "brand_name"), 
                    measure.vars = c("price", "quantity", "promotion"))

# Step 2: Use dcast() to reshape the data back to wide format
wide_data <- dcast(melted_data, 
                   store_code_uc + week_end ~ variable + brand_name, 
                   value.var = "value")

# View the result
head(wide_data)
```

## Merge store information with the movement data

*Now merge the movement data with the store meta data, in particular with the retailer code, the Scantrack (SMM) market code, and the Scantrack market description. But only with the store meta data where we have a valid retailer code. Hence, we need to remove store data if the retailer code is missing (`NA`). Use the `is.na` function to check if `retailer_code` is `NA` or not.*

```{r}
# Merge movement data with store meta data
# Keep only stores with a non-NA retailer code
movement_merged <- merge(wide_data, stores[!is.na(retailer_code)], 
                           by = "store_code_uc", all.x = TRUE)

# Display the merged data
head(movement_merged)
```

## Create time variables or trends

*A time trend records the progress of time. For example, a time trend at the week-level may equal 1 in the first week in the data, 2 in the second week, etc., whereas a trend at the month-level may equal 1 in the first month, 2 in the second month, etc.*

*I suggest you create a monthly time trend. Use the functions `year` and `month` to extract the year and month components of the week (`week_end`) variable in the movement data (alternatively, you could use the `week` function if you wanted to create a time trend at the week-level). Then, use the following code to create the monthly trend:*

```{r}
# Ensure the week_end is in Date format if it isn't already
# Ensure movement_merged is a data.table
setDT(movement_merged)

movement_merged[, week_end := as.Date(week_end)]

# Extract year and month from week_end
movement_merged[, year := year(week_end)]
movement_merged[, month := month(week_end)]

movement_merged[, month_trend := 12*(year - min(year)) + month]
```

## Remove missing values

Finally, *retain only complete cases*, i.e. rows without missing values:

```{r}

# Remove rows with any missing values
movement_merged <- movement_merged[complete.cases(movement_merged)]

# Display the cleaned data
colSums(is.na(movement_merged))

```

\newpage

# Data inspection

## Observations and geographic coverage

*First, document the number of observations and the number of unique stores in the data.*

```{r}
#Document the number of observations and unique stores
num_observations <- nrow(movement_merged)
num_unique_stores <- uniqueN(movement_merged$store_code_uc)

cat("Number of observations:", num_observations, "\n")
cat("Number of unique stores:", num_unique_stores, "\n")



```

*Second, we assesss if the included stores have broad geographic coverage. We hence create a summary table that records the number of observations for each separate Scantrack market:*

```{r}
# Create a summary table for each Scantrack market (SMM_description)
market_coverage <- movement_merged[, .(n_obs = .N), by = SMM_description]

```

Note the use of the data.table internal `.N`: `.N` is the number of observations, either in the whole data table, or---as in this case---the number of observations within each group defined by the `by =` statement.

\medskip

A convenient way to print a table is provided by the **`kable`** function that is included in the `knitr` package. Please consult the documentation for `kable` to see all options. Particularly useful are the options `col.names`, which is used below, and `digits`, which allows you to set the number of digits to the right of the decimal point.

*Now use `kable` to document the number of observations within each Scantrack market.*

```{r}
# Use kable to print the summary table
kable(market_coverage, col.names = c("Scantrack Market", "No. Observations"))
```

## Price variation

Before estimating the demand models we would like to understand the degree of price variation in the data. Comment on why this is important for a regression analysis such as demand estimation!

**Comment :**

-   We need price variation to observe the changes in demand as price varies, which is crucial to derive price elasticity estimation.

We will predict demand for Tide and Gain. For each of these two brands separately, we would like to visualize the overall degree of price variation across observations, and also the variation in relative prices with respect to the competing brands.

-   *To visualize the (own) price variation, normalize the prices of Tide and Gain by dividing by the average of these prices, and show the histogram of normalized prices.*

```{r}
# Normalize prices for Tide and Gain
movement_merged[, price_Tide_normalized := price_Tide / mean(price_Tide, na.rm = TRUE)]
movement_merged[, price_Gain_normalized := price_Gain / mean(price_Gain, na.rm = TRUE)]

# Plot histograms for normalized prices
ggplot(movement_merged, aes(x = price_Tide_normalized)) +
  geom_histogram(binwidth = 0.05, fill = "blue", alpha = 0.7) +
  labs(title = "Histogram of Normalized Tide Prices", x = "Normalized Price (Tide)", y = "Count") +
  scale_x_continuous(limits = c(0.5, 1.5))  # Set limits to avoid extreme outliers

ggplot(movement_merged, aes(x = price_Gain_normalized)) +
  geom_histogram(binwidth = 0.05, fill = "red", alpha = 0.7) +
  labs(title = "Histogram of Normalized Gain Prices", x = "Normalized Price (Gain)", y = "Count") +
  scale_x_continuous(limits = c(0.5, 1.5))  # Set limits to avoid extreme outliers

```

-   *To visualize relative prices, calculate the ratio of Tide and Gain prices with respect to the three competing brands, and show the histogram of relative prices.*

```{r}
# Calculate relative prices for Tide and Gain compared to the competing brands (e.g., ArmHammer, Purex)
movement_merged[, relative_price_Tide_ArmHammer := price_Tide / price_ArmHammer]
movement_merged[, relative_price_Tide_Purex := price_Tide / price_Purex]
movement_merged[, relative_price_Gain_ArmHammer := price_Gain / price_ArmHammer]
movement_merged[, relative_price_Gain_Purex := price_Gain / price_Purex]

# Plot histograms for relative prices
ggplot(movement_merged, aes(x = relative_price_Tide_ArmHammer)) +
  geom_histogram(binwidth = 0.05, fill = "blue", alpha = 0.7) +
  labs(title = "Histogram of Relative Price (Tide/ArmHammer)", x = "Relative Price (Tide/ArmHammer)", y = "Count") +
  scale_x_continuous(limits = c(0.5, 2))  # Set limits to avoid extreme outliers

ggplot(movement_merged, aes(x = relative_price_Tide_Purex)) +
  geom_histogram(binwidth = 0.05, fill = "green", alpha = 0.7) +
  labs(title = "Histogram of Relative Price (Tide/Purex)", x = "Relative Price (Tide/Purex)", y = "Count") +
  scale_x_continuous(limits = c(0.5, 2))  # Set limits to avoid extreme outliers

ggplot(movement_merged, aes(x = relative_price_Gain_ArmHammer)) +
  geom_histogram(binwidth = 0.05, fill = "red", alpha = 0.7) +
  labs(title = "Histogram of Relative Price (Gain/ArmHammer)", x = "Relative Price (Gain/ArmHammer)", y = "Count") +
  scale_x_continuous(limits = c(0.5, 2))  # Set limits to avoid extreme outliers

ggplot(movement_merged, aes(x = relative_price_Gain_Purex)) +
  geom_histogram(binwidth = 0.05, fill = "purple", alpha = 0.7) +
  labs(title = "Histogram of Relative Price (Gain/Purex)", x = "Relative Price (Gain/Purex)", y = "Count") +
  scale_x_continuous(limits = c(0.5, 2))  # Set limits to avoid extreme outliers

```

Note: To avoid that the scale of a graph is distorted by a few outliers, use the `limits` option in `scale_x_continuous` (see the ggplot2 introduction). This also helps to make the graphs comparable with each other.

## Summary of data inspection

*Discuss the data description, including sample size, geographic coverage, and the results on own and relative price variation.*

1.  Sample Size and Unique Stores The dataset includes 1,259,352 observations and covers 6,421 unique stores. The large sample size and number of unique stores ensure a comprehensive dataset that captures diverse pricing and demand patterns across numerous retail locations. This extensive coverage enhances the robustness of our analysis.

2.  Geographic Coverage The dataset encompasses a broad range of Scantrack markets, indicating good geographic representation. The number of observations per market varies, covering major regions like Denver, Northern California, and West Texas, among others. This wide coverage enables us to study regional differences in consumer behavior, competitive strategies, and pricing variations.

3.  Own Price Variation The histograms of normalized prices for Tide and Gain reveal meaningful price variation around their respective means. The histogram for Tide indicates a concentrated price range between approximately 0.75 and 1.25, with the majority of prices clustered around the mean. This distribution suggests consistent pricing with occasional adjustments, likely due to regional factors or promotions.

Similarly, the histogram for Gain shows a comparable spread of prices, but with some observable peaks, indicating periods of price adjustments or strategic promotions. These variations in own prices are essential for the demand analysis as they provide the basis for estimating the sensitivity of demand to price changes.

4.  Relative Price Variation The histograms of relative prices between Tide and its competitors, Arm & Hammer and Purex, illustrate a spread of price ratios centered around 1.0. This indicates that Tide’s prices are generally comparable to its competitors but occasionally deviate, suggesting pricing strategies that reflect competitive positioning.

The histograms of relative prices for Gain with respect to Arm & Hammer and Purex display similar patterns, with price ratios centered around 1.0. However, there are more pronounced peaks in the distributions, implying more aggressive pricing or promotions for Gain compared to its competitors. This relative price variation is

\newpage

# Estimation

Now we are ready to estimate demand models for Tide and Gain.

We want to estimate a sequence of models with an increasing number of controls and compare the stability of the key results across these models. In all models the output is `log(1+quantity_<brand name>)`.

\bigskip

To keep things simple, we will initially estimate demand for Tide only.

Let's start with the following models:

1.  log of own price as only input
2.  Add store fixed effects
3.  Add a time trend---maybe linear, or a polynomial with higher-order terms
4.  Instead of a time trend add fixed effects for each month (more precisely: for each year/month combination)

*Estimate the models using the `feols` function from the fixest package (consult the corresponding fixest guide included among the R learning resources on Canvas). Store the regression outputs in some appropriately named variables (objects).*

\bigskip

**Hint**: Recall that it is perfectly legitimate in R to write model formulas such as

```         
log(1+quantity_<brand name>) ~ log(price_<brand name>)
```

Hence, there is no need to create new variables such as the logarithm of own price, etc., before estimating a demand model.

```{r}
# Model 1: Log of Tide's price as the only input
fit_base <- feols(log(1 + quantity_Tide) ~ log(price_Tide), data = movement_merged)

# Model 2: Add store fixed effects
fit_store_FE <- feols(log(1 + quantity_Tide) ~ log(price_Tide) | store_code_uc, data = movement_merged)

# Model 3: Add a time trend
fit_trend <- feols(log(1 + quantity_Tide) ~ log(price_Tide) + month_trend | store_code_uc, data = movement_merged)

```

```{r}
# Model 4: Add year/month fixed effects

# Create a new variable that combines year and month into a single string
movement_merged[, year_month := paste(year(week_end), month(week_end), sep = "-")]

# Check the new year_month variable
head(movement_merged$year_month)

# Model 4: Log of Tide's price with store fixed effects and year/month fixed effects
fit_month_FE <- feols(log(1 + quantity_Tide) ~ log(price_Tide) | store_code_uc + year_month, data = movement_merged)
```

\bigskip

You can display the regression coefficients using the `summary` function. As a much more elegant solution, however, I recommend using the `etable` function in the `fixest` package, which produces nicely formatted output.

Please **consult the fixest guide on how to use `etable`**, and **go through the** ***Checklist for creating LaTeX tables using `etable`***!

Here is an example (note that the `fit` objects are the regression outputs---adjust the names if necessary):

```{r, results = "asis"}
etable(fit_base, fit_store_FE, fit_trend, fit_month_FE,
       tex = TRUE,
       fitstat = c("n", "r2"), signif.code = NA,
       cluster = c("store_code_uc", "month_trend"))
```

Note the option `cluster = c("store_code_uc", "month_trend")`, which tells `etable` to show standard errors that are clustered at the store and month level. These clustered standard errors will be larger and more accurate than regular standard errors because they reflect that the error terms in the regression are likely correlated at the store and month level.

\bigskip

Before moving on, you may want to remove the regression output objects that are no longer used, because they take up much space in memory:

```{r}
rm(fit_base, fit_store_FE, fit_trend)
```

## Controlling for competitor prices

*Now add the competitor prices to the demand model.*

```{r}
fit_comp <- feols(log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + log(price_Purex) +  log(price_ArmHammer) | store_code_uc + year_month, data = movement_merged)
```

## Controlling for promotions

*Now add the promotions dummies, first just for Tide, then for all brands. Compare the results. Did controlling for promotions change the own price elasticity estimate in an expected manner?*

```{r, results = "asis"}
# Add promotion for tide
fit_promo_comp_tide <-  feols(log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + log(price_ArmHammer) + log(price_Purex) + promotion_Tide  | store_code_uc + year_month, data = movement_merged)

# Add promotion for all brands
fit_promo_comp <-  feols(log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + log(price_ArmHammer) + log(price_Purex) + promotion_Tide + promotion_Gain + promotion_ArmHammer + promotion_Purex | store_code_uc + year_month, data = movement_merged)

etable(fit_month_FE, fit_comp, fit_promo_comp_tide, fit_promo_comp,
       tex = TRUE,
       fitstat = c("n", "r2"), signif.code = NA,
       cluster = c("store_code_uc", "month_trend"))
```

\bigskip

*Summarize and comment on the estimation results. Was it necessary to control for store fixed effects, time trends/fixed effects, as well as competitor prices and promotions? What do we learn from the magnitudes of the own and cross-price elasticities?*

\bigskip

We will use the final model including all variables (I called it `fit_promo_comp`) as our preferred model. To make this final model distinguishable from the regression output for Gain we rename it:

```{r}
fit_Tide = fit_promo_comp
```

1.  Did controlling for promotions change the own price elasticity estimate in an expected manner? Yes, controlling for promotions did change the own price elasticity in an expected manner. Without promotions (Model 1 of the second table), the elasticity of Tide's price is -5.667. After controlling for promotions (Model 4 of the first table), the price elasticity becomes less negative (-4.204), which is expected since promotions tend to increase demand, making the price effect less dramatic when controlling for promotional influences.

2.  Summarize and comment on the estimation results. The estimation results show that the own-price elasticity of Tide is negative across models, indicating price sensitivity. The inclusion of promotions reduces the elasticity, as the promotion increases demand directly. Cross-price elasticities with competitors show that Gain is a significant substitute (elasticity around 0.65 before promo), while Purex has little competitive effect (0.19), and Arm & Hammer also acts as a substitute with a small positive effect (around 0.11). Promotions for Tide significantly increase demand (about 0.38), while competitor promotions have mixed effects.

3.  Was it necessary to control for store fixed effects, time trends/fixed effects, as well as competitor prices and promotions? Yes, controlling for these factors was necessary to improve the accuracy of the estimates. Store fixed effects account for unique store-specific characteristics that could influence demand, while time trends and fixed effects control for temporal variations such as seasonality. Controlling for competitor prices and promotions is also critical to isolate the true effect of Tide’s price and promotion on its quantity sold, ensuring that external competitive factors do not confound the results.

4.  What do we learn from the magnitudes of the own and cross-price elasticities? The own-price elasticity of Tide (-4.20 in the final model) suggests that Tide is quite price-sensitive, with a 1% increase in price leading to about a 4% decrease in demand. The cross-price elasticity with Gain (around 0.65) shows a strong substitutive relationship, meaning that Gain and Tide are significant competitors. Follow by Purex and Arm & Hammer which have a small positive substitutive effect. After Promotion, the cross elasticity is softer between Tide and Gain (0.57), suggesting softer substitution effect. While stronger between Tide and Purex(increases from 0.1938 to 0.2057)/ ArmHammer(increases from 0.1185 to 0.1277). This signified that when Tide is on Promotion, consumers see Purex/ ArmHammer as possible substitute of Tide although still less than Gain. These elasticity highlight the importance of pricing and promotions in driving demand for Tide in a competitive market.

5.  Promotion The coefficient for `promotion_Tide` in Models 3 and 4 is 0.3413 and 0.3418, respectively. This means that when Tide is on promotion, its demand increases by about 34%. This is a substantial effect, indicating that promotions significantly boost the demand for Tide. Model4 promotion effects are smaller but still positive, meaning that when competitors are on promotion, Tide’s demand also increases slightly ie Gain is on promotion, Tide’s demand increases by 2.38%.

\newpage

## Demand model for Gain

*Now repeat the steps to estimate demand for Gain.*

*Briefly comment on the estimates, as you did before with Tide.*

```{r,  results = "asis"}
# Model 1: Log of Tide's price as the only input
fit_base <- feols(log(1 + quantity_Gain) ~ log(price_Gain), data = movement_merged)

# Model 2: Add store fixed effects
fit_store_FE <- feols(log(1 + quantity_Gain) ~ log(price_Gain) | store_code_uc, data = movement_merged)

# Model 3: Add a time trend
fit_trend <- feols(log(1 + quantity_Gain) ~ log(price_Gain) + month_trend | store_code_uc, data = movement_merged)

# Model 4: Add year/month fixed effects
fit_month_FE_gain <- feols(log(1 + quantity_Gain) ~ log(price_Gain) | store_code_uc + year_month, data = movement_merged)

etable(fit_base, fit_store_FE, fit_trend, fit_month_FE_gain,
       tex = TRUE,
       fitstat = c("n", "r2"), signif.code = NA,
       cluster = c("store_code_uc", "month_trend"))

rm(fit_base, fit_store_FE, fit_trend)


### Controlling for competitor prices
fit_comp_gain <- feols(log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + log(price_Purex) +  log(price_ArmHammer) | store_code_uc + year_month, data = movement_merged)

## Controlling for promotions
# Add only promotion dummy for gain
fit_promo_comp_gain <- feols(log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + log(price_Purex) +  log(price_ArmHammer) + promotion_Gain | store_code_uc + year_month, data = movement_merged)

# Add all promotion dummies
fit_promo_comp <-  feols(log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + log(price_ArmHammer) + log(price_Purex) + promotion_Gain + promotion_Tide + promotion_ArmHammer + promotion_Purex | store_code_uc + year_month, data = movement_merged)

fit_Gain = fit_promo_comp

etable(fit_month_FE_gain, fit_comp_gain, fit_promo_comp_gain, fit_promo_comp,
       tex = TRUE,
       fitstat = c("n", "r2"), signif.code = NA,
       cluster = c("store_code_uc", "month_trend"))

```

1.  Did controlling for promotions change the own price elasticity estimate in an expected manner? Yes, controlling for promotions changed the own price elasticity estimate for Gain in an expected manner. Without controlling for promotions (Model 1 of the second table), the own price elasticity of Gain is -6.668, indicating strong price sensitivity. When promotions are controlled for (Model 4 of the second table), the elasticity becomes less negative (-4.969), as expected, since promotions increase demand and thus reduce the overall sensitivity to price changes.

2.  Summarize and comment on the estimation results. The results show that the own-price elasticity of Gain is significantly negative across models, indicating that demand for Gain is highly price-sensitive. When controlling for promotions, the elasticity decreases, meaning that promotions reduce the effect of price on demand. Competitor prices, particularly Tide's price, have a positive impact on Gain’s demand (elasticity around 1.613), suggesting that Tide is a significant substitute. Promotions for Gain itself have a large positive effect (0.69), while competitor promotions slightly increasing Gain demand.

3.  Was it necessary to control for store fixed effects, time trends/fixed effects, as well as competitor prices and promotions? Yes, controlling for store fixed effects, time trends, and competitor prices and promotions was necessary. Store fixed effects account for store-specific characteristics, while time trends control for seasonal or time-based variations. Including competitor prices and promotions helps isolate the true effect of Gain’s price on its quantity demanded by controlling for external competitive factors that could confound the relationship between Gain's price and demand.

4.  What do we learn from the magnitudes of the own and cross-price elasticities? Gain’s own-price elasticity (-6.696 in the final model) shows that demand for Gain is highly price-sensitive, with a 1% increase in price leading to a 6% decrease in demand. The positive cross-price elasticity with Tide (around 1.6) suggests that Gain and Tide are strong substitutes, followed by Purex 1.2. While Arm & Hammer price effect on Gain demand is smaller, suggesting a weak substitute.\
    After promotion, Tide remains a strong substitude for Gain, showing the increase from 1.613 to 1.795. This suggests the substitution is stronger when all promotions included. Tide's price plays a critical role in Gain's demand. Purex is also a substitude for Gain, price elasticity increases in model4 from 0.1217 to 0.1782. ArmHammer is a weak substitute for Gain, but becomes stronger when promotions are included (Model4) from 0.0809 to 0.1481.

\newpage

# Profitability analysis

The goal is to fine-tune prices jointly for Tide and Gain. We hence use the estimates of the preferred demand models and evaluate the product-line profits when we change the prices of the two brands.

\bigskip

To predict profits, let's only retain data for one year, 2013:

```{r}
move_predict = movement_merged[year == 2013]
```

\bigskip

Although we have excellent demand data, we do not know the production costs of the brands (this is confidential information). We can infer the cost making an informed assumption on retail margins and the gross margin of the brand.

```{r}
gross_margin  = 0.35
retail_margin = 0.18

cost_Tide = (1-gross_margin)*(1-retail_margin)*mean(move_predict$price_Tide)
cost_Gain = (1-gross_margin)*(1-retail_margin)*mean(move_predict$price_Gain)

cost_Tide
cost_Gain
```

As prices are measured in dollars per ounce, these marginal costs are also per ounce.

\bigskip

Now create a vector indicating the percentage price changes that we consider within an acceptable range, up to +/- 5%.

```{r}
percentage_delta = seq(-0.05, 0.05, 0.025)    # Identical to = c(-0.5, -0.025, 0.0, 0.025, 0.05)
percentage_delta
```

\bigskip

We will consider all possible combinations of price changes for Tide and Gain. This can be easily achieved by creating a data table with the possible combinations in rows (please look at the documentation for the `rep` function):

```{r}
L = length(percentage_delta)
profit_DT = data.table(delta_Tide = rep(percentage_delta, each = L),
                       delta_Gain = rep(percentage_delta, times = L),
                       profit     = rep(0, times = L*L))

head(profit_DT)
```

Inspect the resulting table. The `profit` column will allow us to store the predicted profits.

\bigskip

Now we are ready to iterate over each row in `profit_DT` and evaluate the total product-line profits of Tide and Gain for the corresponding percentage price changes. You can perform this iteration with a simple for-loop:

```{r}
# Store the original average price of Tide and Gain
orig_price_Tide <- mean(move_predict$price_Tide, na.rm = TRUE)
orig_price_Gain <- mean(move_predict$price_Gain, na.rm = TRUE)

# Print the original prices
orig_price_Tide # 0.1519766
orig_price_Gain # 0.1290901
```

```{r}
# Function to calculate total profit for a given price, cost, and quantity
calc_profit <- function(price, cost, quantity) {
  profit = (price - cost) * quantity
  return(profit)
}

```

```{r}
for (i in 1:nrow(profit_DT)) {
   # Perform profit calculations for the price changes indicated in row i of the profit_DT table
  # Calculate new prices for Tide and Gain based on the percentage change
  move_predict_copy <- copy(move_predict)
  
  new_price_Tide <- move_predict$price_Tide * (1 + profit_DT$delta_Tide[i])
  new_price_Gain <- orig_price_Gain * (1 + profit_DT$delta_Gain[i])
  
  move_predict_copy$price_Tide <- new_price_Tide
  move_predict_copy$price_Gain <- new_price_Gain
  
  # Predict new demand for Tide and Gain using the updated prices
  move_predict_copy[, predicted_quantity_Tide := exp(predict(fit_Tide, newdata = move_predict_copy))]
  move_predict_copy[, predicted_quantity_Gain := exp(predict(fit_Gain, newdata = move_predict_copy))]
  
  # Calculate profits for Tide and Gain
  profit_Tide <- calc_profit(move_predict_copy$price_Tide, cost_Tide, move_predict_copy$predicted_quantity_Tide)
  profit_Gain <- calc_profit(move_predict_copy$price_Gain, cost_Gain, move_predict_copy$predicted_quantity_Gain)
  
  # Store total product-line profits (Tide + Gain) in profit_DT
  profit_DT$profit[i] <- sum(profit_Tide, profit_Gain, na.rm = TRUE)
  print(profit_DT$profit[i])
}

```

```{r}
# Base Total Profit
  move_predict_copy <- copy(move_predict)
  i = 13
  move_predict_copy$price_Tide <- move_predict$price_Tide * (1 + profit_DT$delta_Tide[i])
  move_predict_copy$price_Gain <- orig_price_Gain * (1 + profit_DT$delta_Gain[i])
  
  move_predict_copy[, predicted_quantity_Tide := exp(predict(fit_Tide, newdata = move_predict_copy))] #1696636233
  move_predict_copy[, predicted_quantity_Gain := exp(predict(fit_Gain, newdata = move_predict_copy))]
  
  # Calculate profits for Tide and Gain
  profit_Tide <- calc_profit(move_predict_copy$price_Tide, cost_Tide, move_predict_copy$predicted_quantity_Tide)
  profit_Gain <- calc_profit(move_predict_copy$price_Gain, cost_Gain, move_predict_copy$predicted_quantity_Gain)
  
  #sum(move_predict_copy$predicted_quantity_Tide) #1870104585
  #sum(move_predict_copy$predicted_quantity_Gain) #265303723 
  
  # Store total product-line profits (Tide + Gain) in profit_DT
  baseline_total_profit <- sum(profit_Tide, profit_Gain, na.rm = TRUE)
  # Print baseline total profit
  print(baseline_total_profit)
  
  # Calculate profit ratio relative to baseline profit
  profit_DT[, profit_ratio := profit / baseline_total_profit]
```

\medskip

Some hints:

-   Before you start the loop, store the original price levels of Tide and Gain.
-   Update the price columns in `move_predict` and then predict demand.
-   Calculate total profits at the new price levels for both brands and then store the total profit from Tide and Gain in `profit_DT`.

\medskip

Show a table of profits in levels and in ratios relative to the baseline profit at current price levels, in order to assess the percent profit differences resulting from the contemplated price changes.

```{r}
# Display the table with profit levels and ratios
kable(profit_DT, col.names = c("Delta Tide", "Delta Gain", "Profit Level", "Profit Ratio"), digits = 3)
```

85238164

```{r}
# Install and load necessary packages (if not already installed)
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(reshape2)) install.packages("reshape2")

library(ggplot2)
library(reshape2)

# Reshape the data to wide format for heatmap
profit_wide <- dcast(profit_DT, delta_Tide ~ delta_Gain, value.var = "profit_ratio")

# Melt the data back into a long format for ggplot
profit_long <- melt(profit_wide, id.vars = "delta_Tide", variable.name = "delta_Gain", value.name = "profit_ratio")

# Create the heatmap using ggplot2 with smaller text sizes
ggplot(profit_long, aes(x = delta_Gain, y = delta_Tide, fill = profit_ratio)) +
  geom_tile() +
  scale_fill_gradient(low = "yellow", high = "blue") +
  geom_text(aes(label = round(profit_ratio, 3)), color = "black", size = 3) +  # Display profit_ratio values on the tiles with smaller text
  labs(title = "Profit Ratio Heatmap by Delta Tide and Delta Gain",
       x = "delta_Gain",
       y = "delta_Tide",
       fill = "Profit Ratio") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 10, hjust = 0.5),  # Adjust title size
    axis.title.x = element_text(size = 8),  # Adjust x-axis title size
    axis.title.y = element_text(size = 8),  # Adjust y-axis title size
    axis.text.x = element_text(size = 5),    # Adjust x-axis tick label size
    axis.text.y = element_text(size = 5)     # Adjust y-axis tick label size
  )
```

\bigskip

*Discuss the profitability predictions and how prices should be changed, if at all. How do you reconcile the recommended price changes with the own-price elasticity estimates?*

1\. Data :

-   Baseline scenario : Profit Ratio: 1.
    -   Ratio is 1, the baseline profit when no price changes.
-   Best Profit : Tide increases by 5% and Gain Price increase by 5% (delta_Tide = -0.050 and delta_Gain = +0.050)
    -   Profit ratio is 1.059, signifies 5.9% increase in profit relative to the baseline. Suggesting that consumers buys more Tide and steal demand from Gain. Overall increasing total profit of both products. This combination is better than reduceing price for both products.
-   Most Loss : Both Tide and Gain Price increase 5% (delta_Tide = +0.050 and delta_Gain = +0.050)
    -   Ratio 0.949, which is 5.1% reduction in profit compared to the baseline.
-   When Tide’s price is reduced by 5% (delta Tide = -0.05) and Gain’s price remains unchanged (delta Gain = 0.00), the profit ratio is 1.051, indicating a 5.1% increase in profit. This shows that customers buy more Tide in response to the price reduction, but it does not appear to significantly reduce Gain’s sales.
    -   When Tide’s price is reduced, customers tend to buy more of Tide, but this doesn’t necessarily mean they are switching from Gain. This suggests that the demand for Tide is highly elastic—meaning (\~ -5 own-elasticity) that consumers are very responsive to price reductions for Tide, and they buy more of it when the price goes down.
    -   However, Gain's demand doesn’t drop significantly when Tide’s price is reduced, indicating that Gain’s customer base is less sensitive to Tide’s price changes. Gain and Tide may serve slightly different customer segments, or Gain’s customers may be less inclined to switch products based on Tide’s pricing alone.

2.  Suggestion :

    -   Reduce Prices: The most profitable strategy is to implement a promotion to reduce prices of Tide 5% and increase price of Gain by 5%. This yields in a 5.9% increase in profit, the highest observed.

    <!-- -->

    -   Avoid Price Hikes: Increasing prices, even slightly, results in profit losses due to the strong price elasticity of both brands.

<!-- -->
